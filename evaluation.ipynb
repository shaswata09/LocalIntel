{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaswata/anaconda3/envs/localintel_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.model_utils import get_gpt3_5_completion, get_mistral_7b_completion, get_qwen_7b_completion, get_westlake_7b_completion, get_westseverus_7b_completion, get_llama2_7b_completion, get_aimaven_prometheus_7b_completion, get_llama_31_8b_completion, get_mistral_nemo_12b_instruct_completion, get_mistral_nemo_minitron_8b_base_completion, get_gpt_4o_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_file_path = \"./data/evaluation/LocalIntel_mistral_nemo_minitron_8b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_excel(eval_dataset_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Version</th>\n",
       "      <th>CVE</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>Ground_truth</th>\n",
       "      <th>gpt_35_completion</th>\n",
       "      <th>mistral_7b_completion</th>\n",
       "      <th>qwen_7b_completion</th>\n",
       "      <th>westlake_7b_completion</th>\n",
       "      <th>westseverus_7b_completion</th>\n",
       "      <th>llama_7b_completions</th>\n",
       "      <th>prometheus_7b_completions</th>\n",
       "      <th>llama_31_8b_completions</th>\n",
       "      <th>mistral_nemo_minitron_8b_completions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Langchain</td>\n",
       "      <td>0.0.194</td>\n",
       "      <td>CVE-2023-38896</td>\n",
       "      <td>Considering the use of LangChain's chains for ...</td>\n",
       "      <td>Langchain, chains, implementing, Program-Aided...</td>\n",
       "      <td>...</td>\n",
       "      <td>The vulnerability in LangChain version 0.0.194...</td>\n",
       "      <td>The global_knowledge and local_knowledge do no...</td>\n",
       "      <td>\\nThe information provided indicates a vulnera...</td>\n",
       "      <td>The LangChain chains used in the chatbot appli...</td>\n",
       "      <td>\\nThe global_knowledge provides information ab...</td>\n",
       "      <td>Given the global_knowledge, a vulnerability ex...</td>\n",
       "      <td>\\nBased on the information provided in the glo...</td>\n",
       "      <td>\\nThe given threat or vulnerability report hig...</td>\n",
       "      <td>An issue in LangChain v.0.0.194 and before all...</td>\n",
       "      <td>The LangChain's chains used for implementing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Langchain</td>\n",
       "      <td>0.0.194</td>\n",
       "      <td>CVE-2023-38896</td>\n",
       "      <td>How critical is the langchain vulnerability in...</td>\n",
       "      <td>critical, langchain vulnerability, potential i...</td>\n",
       "      <td>...</td>\n",
       "      <td>This LangChain vulnerability is critical due t...</td>\n",
       "      <td>\\nThe langchain vulnerability is highly critic...</td>\n",
       "      <td>\\nFrom the global_knowledge provided, a remote...</td>\n",
       "      <td>The langchain vulnerability in Harrison Chase ...</td>\n",
       "      <td>\\nThe given threat or vulnerability report hig...</td>\n",
       "      <td>The given threat in Harrison Chase langchain e...</td>\n",
       "      <td>Based on the information provided in global_k...</td>\n",
       "      <td>\\nThe identified vulnerability in Harrison Cha...</td>\n",
       "      <td>The vulnerability in Harrison Chase langchain ...</td>\n",
       "      <td>The langchain vulnerability is critical in te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>StreamLit</td>\n",
       "      <td>0.63.0</td>\n",
       "      <td>CVE-2023-27494</td>\n",
       "      <td>What potential consequences could arise from t...</td>\n",
       "      <td>potential consequences, exploitation, XSS vuln...</td>\n",
       "      <td>...</td>\n",
       "      <td>Users of hosted Streamlit app(s) are vulnerabl...</td>\n",
       "      <td>\\nIf the XSS vulnerability in Streamlit versio...</td>\n",
       "      <td>\\nAn attacker could inject malicious scripts i...</td>\n",
       "      <td>The potential consequences from the exploitati...</td>\n",
       "      <td>\\nIn the exploitation of the XSS vulnerability...</td>\n",
       "      <td>In the given context, the chatbot application ...</td>\n",
       "      <td>\\nBased on the information provided in global_...</td>\n",
       "      <td>\\nExploitation of the XSS vulnerability in Str...</td>\n",
       "      <td>More information is required to answer the que...</td>\n",
       "      <td>The potential consequences of the XSS vulnera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>StreamLit</td>\n",
       "      <td>0.63.0</td>\n",
       "      <td>CVE-2023-27494</td>\n",
       "      <td>What actions can an attacker take advantage of...</td>\n",
       "      <td>attacker, actions, advantage, exploitation, vu...</td>\n",
       "      <td>...</td>\n",
       "      <td>Through the exploitation of the vulnerability ...</td>\n",
       "      <td>\\nAn attacker could trick a user into visiting...</td>\n",
       "      <td>\\nAn attacker could create a malicious URL wit...</td>\n",
       "      <td>An attacker could exploit the cross-site scrip...</td>\n",
       "      <td>\\nAn attacker can take advantage of the vulner...</td>\n",
       "      <td>An attacker, leveraging the cross-site scripti...</td>\n",
       "      <td>\\nBased on the information provided in the glo...</td>\n",
       "      <td>\\nAn attacker taking advantage of the exploite...</td>\n",
       "      <td>An attacker can craft a malicious URL with Jav...</td>\n",
       "      <td>An attacker can craft a malicious URL with Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Docker</td>\n",
       "      <td>3.1</td>\n",
       "      <td>CVE-2023-0626</td>\n",
       "      <td>How does the vulnerability in Docker Desktop p...</td>\n",
       "      <td>vulnerability, Docker Desktop, impact, securit...</td>\n",
       "      <td>...</td>\n",
       "      <td>The vulnerability in Docker Desktop before ver...</td>\n",
       "      <td>The vulnerability in Docker Desktop potentiall...</td>\n",
       "      <td>\\nGiven the information provided in global_kno...</td>\n",
       "      <td>The vulnerability in Docker Desktop before 4.1...</td>\n",
       "      <td>\\nThe given vulnerability in Docker Desktop be...</td>\n",
       "      <td>\\nThe vulnerability in Docker Desktop before 4...</td>\n",
       "      <td>\\nThe vulnerability in Docker Desktop could po...</td>\n",
       "      <td>\\nThe vulnerability in Docker Desktop before 4...</td>\n",
       "      <td>More information is required to answer the que...</td>\n",
       "      <td>The vulnerability in Docker Desktop before ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0             0             0           0   \n",
       "1             1             1             1             1           1   \n",
       "2             2             2             2             2           2   \n",
       "3             3             3             3             3           3   \n",
       "4             4             4             4             4           4   \n",
       "\n",
       "  Dependency  Version             CVE  \\\n",
       "0  Langchain  0.0.194  CVE-2023-38896   \n",
       "1  Langchain  0.0.194  CVE-2023-38896   \n",
       "2  StreamLit   0.63.0  CVE-2023-27494   \n",
       "3  StreamLit   0.63.0  CVE-2023-27494   \n",
       "4     Docker      3.1   CVE-2023-0626   \n",
       "\n",
       "                                           Questions  \\\n",
       "0  Considering the use of LangChain's chains for ...   \n",
       "1  How critical is the langchain vulnerability in...   \n",
       "2  What potential consequences could arise from t...   \n",
       "3  What actions can an attacker take advantage of...   \n",
       "4  How does the vulnerability in Docker Desktop p...   \n",
       "\n",
       "                                            Keywords  ...  \\\n",
       "0  Langchain, chains, implementing, Program-Aided...  ...   \n",
       "1  critical, langchain vulnerability, potential i...  ...   \n",
       "2  potential consequences, exploitation, XSS vuln...  ...   \n",
       "3  attacker, actions, advantage, exploitation, vu...  ...   \n",
       "4  vulnerability, Docker Desktop, impact, securit...  ...   \n",
       "\n",
       "                                        Ground_truth  \\\n",
       "0  The vulnerability in LangChain version 0.0.194...   \n",
       "1  This LangChain vulnerability is critical due t...   \n",
       "2  Users of hosted Streamlit app(s) are vulnerabl...   \n",
       "3  Through the exploitation of the vulnerability ...   \n",
       "4  The vulnerability in Docker Desktop before ver...   \n",
       "\n",
       "                                   gpt_35_completion  \\\n",
       "0  The global_knowledge and local_knowledge do no...   \n",
       "1  \\nThe langchain vulnerability is highly critic...   \n",
       "2  \\nIf the XSS vulnerability in Streamlit versio...   \n",
       "3  \\nAn attacker could trick a user into visiting...   \n",
       "4  The vulnerability in Docker Desktop potentiall...   \n",
       "\n",
       "                               mistral_7b_completion  \\\n",
       "0  \\nThe information provided indicates a vulnera...   \n",
       "1  \\nFrom the global_knowledge provided, a remote...   \n",
       "2  \\nAn attacker could inject malicious scripts i...   \n",
       "3  \\nAn attacker could create a malicious URL wit...   \n",
       "4  \\nGiven the information provided in global_kno...   \n",
       "\n",
       "                                  qwen_7b_completion  \\\n",
       "0  The LangChain chains used in the chatbot appli...   \n",
       "1  The langchain vulnerability in Harrison Chase ...   \n",
       "2  The potential consequences from the exploitati...   \n",
       "3  An attacker could exploit the cross-site scrip...   \n",
       "4  The vulnerability in Docker Desktop before 4.1...   \n",
       "\n",
       "                              westlake_7b_completion  \\\n",
       "0  \\nThe global_knowledge provides information ab...   \n",
       "1  \\nThe given threat or vulnerability report hig...   \n",
       "2  \\nIn the exploitation of the XSS vulnerability...   \n",
       "3  \\nAn attacker can take advantage of the vulner...   \n",
       "4  \\nThe given vulnerability in Docker Desktop be...   \n",
       "\n",
       "                           westseverus_7b_completion  \\\n",
       "0  Given the global_knowledge, a vulnerability ex...   \n",
       "1  The given threat in Harrison Chase langchain e...   \n",
       "2  In the given context, the chatbot application ...   \n",
       "3  An attacker, leveraging the cross-site scripti...   \n",
       "4  \\nThe vulnerability in Docker Desktop before 4...   \n",
       "\n",
       "                                llama_7b_completions  \\\n",
       "0  \\nBased on the information provided in the glo...   \n",
       "1   Based on the information provided in global_k...   \n",
       "2  \\nBased on the information provided in global_...   \n",
       "3  \\nBased on the information provided in the glo...   \n",
       "4  \\nThe vulnerability in Docker Desktop could po...   \n",
       "\n",
       "                           prometheus_7b_completions  \\\n",
       "0  \\nThe given threat or vulnerability report hig...   \n",
       "1  \\nThe identified vulnerability in Harrison Cha...   \n",
       "2  \\nExploitation of the XSS vulnerability in Str...   \n",
       "3  \\nAn attacker taking advantage of the exploite...   \n",
       "4  \\nThe vulnerability in Docker Desktop before 4...   \n",
       "\n",
       "                             llama_31_8b_completions  \\\n",
       "0  An issue in LangChain v.0.0.194 and before all...   \n",
       "1  The vulnerability in Harrison Chase langchain ...   \n",
       "2  More information is required to answer the que...   \n",
       "3  An attacker can craft a malicious URL with Jav...   \n",
       "4  More information is required to answer the que...   \n",
       "\n",
       "                mistral_nemo_minitron_8b_completions  \n",
       "0   The LangChain's chains used for implementing ...  \n",
       "1   The langchain vulnerability is critical in te...  \n",
       "2   The potential consequences of the XSS vulnera...  \n",
       "3   An attacker can craft a malicious URL with Ja...  \n",
       "4   The vulnerability in Docker Desktop before ve...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Version</th>\n",
       "      <th>CVE</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>Ground_truth</th>\n",
       "      <th>gpt_35_completion</th>\n",
       "      <th>mistral_7b_completion</th>\n",
       "      <th>qwen_7b_completion</th>\n",
       "      <th>westlake_7b_completion</th>\n",
       "      <th>westseverus_7b_completion</th>\n",
       "      <th>llama_7b_completions</th>\n",
       "      <th>prometheus_7b_completions</th>\n",
       "      <th>llama_31_8b_completions</th>\n",
       "      <th>mistral_nemo_minitron_8b_completions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>JSON5</td>\n",
       "      <td>1.0.1</td>\n",
       "      <td>CVE-2022-46175</td>\n",
       "      <td>How can an attacker target our application by ...</td>\n",
       "      <td>target, attacker, application, passing, specia...</td>\n",
       "      <td>...</td>\n",
       "      <td>The parse method of the JSON5 library before a...</td>\n",
       "      <td>\\nThe attacker can target the application by p...</td>\n",
       "      <td>\\nAn attacker could potentially exploit the vu...</td>\n",
       "      <td>An attacker can target your application by pas...</td>\n",
       "      <td>\\nAn attacker can target your application by p...</td>\n",
       "      <td>An attacker targeting the application, exploit...</td>\n",
       "      <td>\\nAn attacker can target your application by p...</td>\n",
       "      <td>\\nAn attacker could exploit the vulnerability ...</td>\n",
       "      <td>An attacker can target your application by pas...</td>\n",
       "      <td>The attacker can target our application by pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>57</td>\n",
       "      <td>webpack</td>\n",
       "      <td>5.89.0</td>\n",
       "      <td>CVE-2022-37601</td>\n",
       "      <td>Given Toy Browser uses webpack library, can an...</td>\n",
       "      <td>Toy Browser, webpack library, attacker, protot...</td>\n",
       "      <td>...</td>\n",
       "      <td>The Webpack library loader-utils 2.0.0 contain...</td>\n",
       "      <td>Yes, an attacker can perform prototype polluti...</td>\n",
       "      <td>\\nBased on the provided global_knowledge and l...</td>\n",
       "      <td>Based on the information provided, it is possi...</td>\n",
       "      <td>\\nConsidering the given global_knowledge about...</td>\n",
       "      <td>Considering the global_knowledge, there's a pr...</td>\n",
       "      <td>Based on the information provided in the glob...</td>\n",
       "      <td>\\nConsidering the given global_knowledge, prot...</td>\n",
       "      <td>I do not know the answer.</td>\n",
       "      <td>No\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>58</td>\n",
       "      <td>terser</td>\n",
       "      <td>5.14.0</td>\n",
       "      <td>CVE-2022-25858</td>\n",
       "      <td>How can an adversary use regular expression to...</td>\n",
       "      <td>adversary, regular expression, exploit, applic...</td>\n",
       "      <td>...</td>\n",
       "      <td>Toy browser uses terser package with version 5...</td>\n",
       "      <td>\\nAn adversary can exploit your application by...</td>\n",
       "      <td>\\nBased on the global_knowledge provided, an a...</td>\n",
       "      <td>An adversary can exploit your application usin...</td>\n",
       "      <td>\\nAn adversary attempting to exploit your appl...</td>\n",
       "      <td>more information is required to explain the e...</td>\n",
       "      <td>I cannot answer your question based on the in...</td>\n",
       "      <td>\\nAn adversary taking advantage of the vulnera...</td>\n",
       "      <td>More information is required to answer the que...</td>\n",
       "      <td>More information is required to answer the qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>59</td>\n",
       "      <td>redux</td>\n",
       "      <td>4.1.0</td>\n",
       "      <td>CVE-2023-30751</td>\n",
       "      <td>Given Toy Browser application uses redux frame...</td>\n",
       "      <td>Toy Browser, application, redux framework, red...</td>\n",
       "      <td>...</td>\n",
       "      <td>The Redux framework has been identified as vul...</td>\n",
       "      <td>Yes, an adversary can perform XSS as the iCont...</td>\n",
       "      <td>\\nBased on the provided global_knowledge and l...</td>\n",
       "      <td>Yes, an adversary could potentially perform Cr...</td>\n",
       "      <td>\\nThe given global_knowledge talks about a vul...</td>\n",
       "      <td>\\nMore information is required to determine if...</td>\n",
       "      <td>Based on the information provided in global_k...</td>\n",
       "      <td>\\nMore information is required to determine if...</td>\n",
       "      <td>I do not know the answer.</td>\n",
       "      <td>No, because the application uses redux framew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>redux</td>\n",
       "      <td>4.1.0</td>\n",
       "      <td>CVE-2023-30751</td>\n",
       "      <td>Is it possible for an adversary to predict the...</td>\n",
       "      <td>possible, adversary, predict, MD5 hash value, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>The Redux Framework plugin, before version 4.2...</td>\n",
       "      <td>Yes, it is possible for an adversary to predic...</td>\n",
       "      <td>\\nBased on the provided global_knowledge and l...</td>\n",
       "      <td>No, it is not possible for an adversary to pre...</td>\n",
       "      <td>\\nMore information is required to answer the q...</td>\n",
       "      <td>\\nMore information is required to answer the q...</td>\n",
       "      <td>\\nI cannot answer your question based on the i...</td>\n",
       "      <td>\\nMore information is required to answer the q...</td>\n",
       "      <td>I do not know the answer.</td>\n",
       "      <td>No, it is not possible for an adversary to pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "53            53            53            53            53          56   \n",
       "54            54            54            54            54          57   \n",
       "55            55            55            55            55          58   \n",
       "56            56            56            56            56          59   \n",
       "57            57            57            57            57          60   \n",
       "\n",
       "   Dependency Version             CVE  \\\n",
       "53      JSON5  1.0.1   CVE-2022-46175   \n",
       "54    webpack  5.89.0  CVE-2022-37601   \n",
       "55     terser  5.14.0  CVE-2022-25858   \n",
       "56      redux   4.1.0  CVE-2023-30751   \n",
       "57      redux   4.1.0  CVE-2023-30751   \n",
       "\n",
       "                                            Questions  \\\n",
       "53  How can an attacker target our application by ...   \n",
       "54  Given Toy Browser uses webpack library, can an...   \n",
       "55  How can an adversary use regular expression to...   \n",
       "56  Given Toy Browser application uses redux frame...   \n",
       "57  Is it possible for an adversary to predict the...   \n",
       "\n",
       "                                             Keywords  ...  \\\n",
       "53  target, attacker, application, passing, specia...  ...   \n",
       "54  Toy Browser, webpack library, attacker, protot...  ...   \n",
       "55  adversary, regular expression, exploit, applic...  ...   \n",
       "56  Toy Browser, application, redux framework, red...  ...   \n",
       "57  possible, adversary, predict, MD5 hash value, ...  ...   \n",
       "\n",
       "                                         Ground_truth  \\\n",
       "53  The parse method of the JSON5 library before a...   \n",
       "54  The Webpack library loader-utils 2.0.0 contain...   \n",
       "55  Toy browser uses terser package with version 5...   \n",
       "56  The Redux framework has been identified as vul...   \n",
       "57  The Redux Framework plugin, before version 4.2...   \n",
       "\n",
       "                                    gpt_35_completion  \\\n",
       "53  \\nThe attacker can target the application by p...   \n",
       "54  Yes, an attacker can perform prototype polluti...   \n",
       "55  \\nAn adversary can exploit your application by...   \n",
       "56  Yes, an adversary can perform XSS as the iCont...   \n",
       "57  Yes, it is possible for an adversary to predic...   \n",
       "\n",
       "                                mistral_7b_completion  \\\n",
       "53  \\nAn attacker could potentially exploit the vu...   \n",
       "54  \\nBased on the provided global_knowledge and l...   \n",
       "55  \\nBased on the global_knowledge provided, an a...   \n",
       "56  \\nBased on the provided global_knowledge and l...   \n",
       "57  \\nBased on the provided global_knowledge and l...   \n",
       "\n",
       "                                   qwen_7b_completion  \\\n",
       "53  An attacker can target your application by pas...   \n",
       "54  Based on the information provided, it is possi...   \n",
       "55  An adversary can exploit your application usin...   \n",
       "56  Yes, an adversary could potentially perform Cr...   \n",
       "57  No, it is not possible for an adversary to pre...   \n",
       "\n",
       "                               westlake_7b_completion  \\\n",
       "53  \\nAn attacker can target your application by p...   \n",
       "54  \\nConsidering the given global_knowledge about...   \n",
       "55  \\nAn adversary attempting to exploit your appl...   \n",
       "56  \\nThe given global_knowledge talks about a vul...   \n",
       "57  \\nMore information is required to answer the q...   \n",
       "\n",
       "                            westseverus_7b_completion  \\\n",
       "53  An attacker targeting the application, exploit...   \n",
       "54  Considering the global_knowledge, there's a pr...   \n",
       "55   more information is required to explain the e...   \n",
       "56  \\nMore information is required to determine if...   \n",
       "57  \\nMore information is required to answer the q...   \n",
       "\n",
       "                                 llama_7b_completions  \\\n",
       "53  \\nAn attacker can target your application by p...   \n",
       "54   Based on the information provided in the glob...   \n",
       "55   I cannot answer your question based on the in...   \n",
       "56   Based on the information provided in global_k...   \n",
       "57  \\nI cannot answer your question based on the i...   \n",
       "\n",
       "                            prometheus_7b_completions  \\\n",
       "53  \\nAn attacker could exploit the vulnerability ...   \n",
       "54  \\nConsidering the given global_knowledge, prot...   \n",
       "55  \\nAn adversary taking advantage of the vulnera...   \n",
       "56  \\nMore information is required to determine if...   \n",
       "57  \\nMore information is required to answer the q...   \n",
       "\n",
       "                              llama_31_8b_completions  \\\n",
       "53  An attacker can target your application by pas...   \n",
       "54                          I do not know the answer.   \n",
       "55  More information is required to answer the que...   \n",
       "56                          I do not know the answer.   \n",
       "57                          I do not know the answer.   \n",
       "\n",
       "                 mistral_nemo_minitron_8b_completions  \n",
       "53   The attacker can target our application by pa...  \n",
       "54                                             No\\n\\n  \n",
       "55   More information is required to answer the qu...  \n",
       "56   No, because the application uses redux framew...  \n",
       "57   No, it is not possible for an adversary to pr...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.886879</td>\n",
       "      <td>16.886879</td>\n",
       "      <td>16.886879</td>\n",
       "      <td>16.886879</td>\n",
       "      <td>18.178453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>30.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.750000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>45.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0\n",
       "count     58.000000     58.000000     58.000000     58.000000   58.000000\n",
       "mean      28.500000     28.500000     28.500000     28.500000   30.000000\n",
       "std       16.886879     16.886879     16.886879     16.886879   18.178453\n",
       "min        0.000000      0.000000      0.000000      0.000000    0.000000\n",
       "25%       14.250000     14.250000     14.250000     14.250000   14.250000\n",
       "50%       28.500000     28.500000     28.500000     28.500000   30.500000\n",
       "75%       42.750000     42.750000     42.750000     42.750000   45.750000\n",
       "max       57.000000     57.000000     57.000000     57.000000   60.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.iloc[[37]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.iloc[[27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.iloc[[26]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = eval_df.drop([37, 27, 26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.886879</td>\n",
       "      <td>16.886879</td>\n",
       "      <td>16.886879</td>\n",
       "      <td>16.886879</td>\n",
       "      <td>18.178453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>30.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.750000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>45.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0\n",
       "count     58.000000     58.000000     58.000000     58.000000   58.000000\n",
       "mean      28.500000     28.500000     28.500000     28.500000   30.000000\n",
       "std       16.886879     16.886879     16.886879     16.886879   18.178453\n",
       "min        0.000000      0.000000      0.000000      0.000000    0.000000\n",
       "25%       14.250000     14.250000     14.250000     14.250000   14.250000\n",
       "50%       28.500000     28.500000     28.500000     28.500000   30.500000\n",
       "75%       42.750000     42.750000     42.750000     42.750000   45.750000\n",
       "max       57.000000     57.000000     57.000000     57.000000   60.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(eval_df['Questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Considering the use of LangChain's chains for implementing Program-Aided Language Models (PAL) within the chatbot, application, how susceptible are these chains to exploitation?\",\n",
       " 'How critical is the langchain vulnerability in terms of potential impact on user interactions and system integrity?',\n",
       " \"What potential consequences could arise from the exploitation of the XSS vulnerability in Streamlit for the chatbot, application's users?\",\n",
       " 'What actions can an attacker take advantage of through the exploitation of the vulnerability in Streamlit?',\n",
       " 'How does the vulnerability in Docker Desktop potentially impact the security of the chatbot, application running in a Linux (Ubuntu 22.04) environment?',\n",
       " 'Can you explain the specific nature of the remote code execution (RCE) vulnerability and how it could be exploited via query parameters in the message-box route within Docker Desktop?',\n",
       " \"How might the vulnerability in GitPython impact the reliability and security of the chatbot, application's interactions with Git repositories?\",\n",
       " 'Could you explain on the specific scenarios within GitPython vulnerability, and how it may impact the chatbot, application?',\n",
       " \"How might an attacker exploit joblib vulnerability to execute arbitrary code within our chatbot application's environment?\",\n",
       " \"What are the potential risks to our chatbot, application's reliability and data integrity pertaining JobLib vulnerability? \",\n",
       " 'How could an attacker exploit the null assertion vulnerability in markdown-it to cause denial of service?',\n",
       " 'What potential impact could DOS due to markdown-it vulnerability have on our chatbot, application and its users?',\n",
       " 'What potential impact could the vulnerability have on the integrity of our TOML configuration data?',\n",
       " 'Could TOML exploitation lead to corruption or loss of important configuration settings, affecting the behavior of our chatbot?',\n",
       " 'What potential risks does TQDM vulnerability pose to the confidentiality, integrity, and availability of our data and services?',\n",
       " 'How could adversaries leverage vulnerabilities in tqdm to target our chatbot, application?',\n",
       " \"How does the vulnerability in NLTK's impact the security and performance of the text processing tasks, such as tokenization and stemming, within our chatbot, application?\",\n",
       " \"What potential risks does NLTK pose to the overall reliability and integrity of our chatbot's functionality, particularly in the context of processing user input and extracting information?\",\n",
       " 'How exactly does the vulnerability in Pandas allow for arbitrary code execution?',\n",
       " 'What specific steps or conditions are required for an attacker to exploit Pandas vulnerability and execute arbitrary code?',\n",
       " \"How does the vulnerability within scipy impact the security of our chatbot, application's statistical analysis tasks, particularly when handling sensitive user data collected through the rating system?\",\n",
       " \"What potential risks does the insecure creation of temporary directories in Scipy's weave component pose to the confidentiality and integrity of user feedback data processed by our chatbot, application?\",\n",
       " \"In what manner does the vulnerability in NumPy affect the security of the chatbot, application's data preprocessing functions, especially when managing user inputs or feedback ratings stored within NumPy arrays?\",\n",
       " 'How can the vulnerability in NumPy affect the overall security posture of our chatbot, application?',\n",
       " \"How is the security of our chatbot, application's image processing tasks impacted by the vulnerability in Pillow?\",\n",
       " \"How might the vulnerability in Pillow affect the overall reliability and functionality of our chatbot, application's user interface, given its reliance on image manipulation for visual elements?\",\n",
       " 'Given that the chatbot, application utilizes PyYAML for parsing configuration files, is there a potential for arbitrary code execution if a malicious actor manages to inject specially crafted YAML content into these files?',\n",
       " 'Are there any security measures in place, such as input validation or sandboxing, to protect against the potential code execution vulnerability when processing YAML configuration files with PyYAML?',\n",
       " 'Given that the chatbot, application utilizes PyArrow for efficient data processing and integration with Pandas, is there a potential for arbitrary code execution if the application processes untrusted data, such as user-supplied input files?',\n",
       " \"What steps should be taken to address the vulnerability in PyArrow to ensure the security of our chatbot, application's data processing functionalities?\",\n",
       " 'What is the potential impact of the c-ares denial-of-service vulnerability on our application?',\n",
       " \"Could the libuv vulnerability allow attackers to bypass our application's security checks and access internal APIs or resources?\",\n",
       " 'What specific HTTP Request Smuggling (HRS) attack scenarios are possible against our application due to the llhttp vulnerability?',\n",
       " 'How feasible is it for an attacker to exploit this vulnerability in our application, considering the requirement of sending repeated large SETTINGS frames?',\n",
       " 'Is our application vulnerable to openssl (node-openssl) package?',\n",
       " 'What is the potential impact of the SpEL Injection vulnerability in the Spring Data MongoDB dependency on our application?',\n",
       " 'Is our application vulnerable to the Denial-of-Service (DoS) vulnerability in Spring MVC?',\n",
       " 'What is the potential impact of the vulnerability of the JSON Web Tokens implementation on our Spring Boot application?',\n",
       " \"What is the potential impact of the modelmapper's com.h2database vulnerability on our application?\",\n",
       " 'What is the potential impact of the vulnerability affecting org.h2.util.JdbcUtils.getConnection method in the H2 database on our Spring Boot application?',\n",
       " 'Is the database vulnerable to CVE-2024-0232?',\n",
       " 'Can the sqlite version be exploitable by remote-code-execution vulnerability?',\n",
       " 'Is the version of g++ vulnerable to buffer-overflow?',\n",
       " 'Can the version of xcode lead to disclosure of user information?',\n",
       " 'Is xcode vulnerable to code injection vulnerability?',\n",
       " 'Does Xcode be exploited to perform XML-external-entity (XXE) attacks?',\n",
       " 'Can libxml2 be exploited to corrupt heap memory?',\n",
       " 'Is libxml2 vulnerable to denial-of-service attack caused due to excessive stack consumption?',\n",
       " 'Can cmake exploit our application through remote-code-execution (RCE)?',\n",
       " 'Is there a vulnerability of cmake that allows local users to gain priviledge of RUNPATH?',\n",
       " 'Is it possible for our application to be accessed through directory traversal using a specially crafted ZIP archive?',\n",
       " \"How could an adversary exploit the jszip package utilized in Toy Browser to arbitrarily modify an object's prototype instance?\",\n",
       " 'Given that the toy browser application utilizes webtorrent, is there a risk of XSS on our HTTP server?',\n",
       " 'How can an attacker target our application by passing specially crafted string through JSON5?',\n",
       " 'Given Toy Browser uses webpack library, can an attacker perform prototype pollution attack?',\n",
       " 'How can an adversary use regular expression to exploit our application using terser package?',\n",
       " 'Given Toy Browser application uses redux framework, can an adversary perform XSS?',\n",
       " 'Is it possible for an adversary to predict the MD5 hash value in order to wrongly authenticate an unauthenticated user?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_knowledge = list(eval_df['Global_Knowledge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An issue in Harrison Chase langchain v.0.0.194 and before allows a remote attacker to execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions.',\n",
       " 'An issue in Harrison Chase langchain v.0.0.194 and before allows a remote attacker to execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions.',\n",
       " 'Streamlit, software for turning data scripts into web applications, had a cross-site scripting (XSS) vulnerability in versions 0.63.0 through 0.80.0. Users of hosted Streamlit app(s) were vulnerable to a reflected XSS vulnerability. An attacker could craft a malicious URL with Javascript payloads to a Streamlit app. The attacker could then trick the user into visiting the malicious URL and, if successful, the server would render the malicious javascript payload as-is, leading to XSS. Version 0.81.0 contains a patch for this vulnerability.',\n",
       " 'Streamlit, software for turning data scripts into web applications, had a cross-site scripting (XSS) vulnerability in versions 0.63.0 through 0.80.0. Users of hosted Streamlit app(s) were vulnerable to a reflected XSS vulnerability. An attacker could craft a malicious URL with Javascript payloads to a Streamlit app. The attacker could then trick the user into visiting the malicious URL and, if successful, the server would render the malicious javascript payload as-is, leading to XSS. Version 0.81.0 contains a patch for this vulnerability.',\n",
       " 'Docker Desktop before 4.12.0 is vulnerable to RCE via query parameters in message-box route. This issue affects Docker Desktop: before 4.12.0.',\n",
       " 'Docker Desktop before 4.12.0 is vulnerable to RCE via query parameters in message-box route. This issue affects Docker Desktop: before 4.12.0.\\n',\n",
       " 'GitPython is a python library used to interact with Git repositories. There is an incomplete fix for CVE-2023-40590. On Windows, GitPython uses an untrusted search path if it uses a shell to run `git`, as well as when it runs `bash.exe` to interpret hooks. If either of those features are used on Windows, a malicious `git.exe` or `bash.exe` may be run from an untrusted repository. This issue has been patched in version 3.1.41.',\n",
       " 'GitPython is a python library used to interact with Git repositories. There is an incomplete fix for CVE-2023-40590. On Windows, GitPython uses an untrusted search path if it uses a shell to run `git`, as well as when it runs `bash.exe` to interpret hooks. If either of those features are used on Windows, a malicious `git.exe` or `bash.exe` may be run from an untrusted repository. This issue has been patched in version 3.1.41.',\n",
       " 'The package joblib from 0 and before 1.2.0 are vulnerable to Arbitrary Code Execution via the pre_dispatch flag in Parallel() class due to the eval() statement.',\n",
       " 'The package joblib from 0 and before 1.2.0 are vulnerable to Arbitrary Code Execution via the pre_dispatch flag in Parallel() class due to the eval() statement.',\n",
       " 'Denial of service could be caused to markdown-it-py, before v2.2.0, if an attacker was allowed to force null assertions with specially crafted input.',\n",
       " 'Denial of service could be caused to markdown-it-py, before v2.2.0, if an attacker was allowed to force null assertions with specially crafted input.',\n",
       " 'Those using jackson-dataformats-text to parse TOML data may be vulnerable to Denial of Service attacks (DOS). If the parser is running on user supplied input, an attacker may supply content that causes the parser to crash by stackoverflow. This effect may support a denial of service attack.',\n",
       " 'Those using jackson-dataformats-text to parse TOML data may be vulnerable to Denial of Service attacks (DOS). If the parser is running on user supplied input, an attacker may supply content that causes the parser to crash by stackoverflow. This effect may support a denial of service attack.',\n",
       " 'The tqdm._version module in tqdm versions 4.4.1 and 4.10 allows local users to execute arbitrary code via a crafted repo with a malicious git log in the current working directory.',\n",
       " 'The tqdm._version module in tqdm versions 4.4.1 and 4.10 allows local users to execute arbitrary code via a crafted repo with a malicious git log in the current working directory.',\n",
       " 'nltk is vulnerable to Inefficient Regular Expression Complexity',\n",
       " 'nltk is vulnerable to Inefficient Regular Expression Complexity',\n",
       " 'GenerateSDFPipeline in synthetic_dataframe in PandasAI (aka pandas-ai) through 1.5.17 allows attackers to trigger the generation of arbitrary Python code that is executed by SDFCodeExecutor. An attacker can create a dataframe that provides an English language specification of this Python code. NOTE: the vendor previously attempted to restrict code execution in response to a separate issue, CVE-2023-39660.',\n",
       " 'GenerateSDFPipeline in synthetic_dataframe in PandasAI (aka pandas-ai) through 1.5.17 allows attackers to trigger the generation of arbitrary Python code that is executed by SDFCodeExecutor. An attacker can create a dataframe that provides an English language specification of this Python code. NOTE: the vendor previously attempted to restrict code execution in response to a separate issue, CVE-2023-39660.',\n",
       " 'The scipy.weave component in SciPy before 0.12.1 creates insecure temporary directories.',\n",
       " 'The scipy.weave component in SciPy before 0.12.1 creates insecure temporary directories.',\n",
       " 'An issue was discovered in NumPy 1.16.0 and earlier. It uses the pickle Python module unsafely, which allows remote attackers to execute arbitrary code via a crafted serialized object, as demonstrated by a numpy.load call. NOTE: third parties dispute this issue because it is a behavior that might have legitimate applications in (for example) loading serialized Python object arrays from trusted and authenticated sources',\n",
       " 'An issue was discovered in NumPy 1.16.0 and earlier. It uses the pickle Python module unsafely, which allows remote attackers to execute arbitrary code via a crafted serialized object, as demonstrated by a numpy.load call. NOTE: third parties dispute this issue because it is a behavior that might have legitimate applications in (for example) loading serialized Python object arrays from trusted and authenticated sources',\n",
       " 'Pillow through 10.1.0 allows PIL.ImageMath.eval Arbitrary Code Execution via the environment parameter, a different vulnerability than CVE-2022-22817 (which was about the expression parameter).',\n",
       " 'Pillow through 10.1.0 allows PIL.ImageMath.eval Arbitrary Code Execution via the environment parameter, a different vulnerability than CVE-2022-22817 (which was about the expression parameter).\\n',\n",
       " 'A vulnerability was discovered in the PyYAML library in versions before 5.4, where it is susceptible to arbitrary code execution when it processes untrusted YAML files through the full_load method or with the FullLoader loader. Applications that use the library to process untrusted input may be vulnerable to this flaw. This flaw allows an attacker to execute arbitrary code on the system by abusing the python/object/new constructor. This flaw is due to an incomplete fix for CVE-2020-1747.',\n",
       " 'A vulnerability was discovered in the PyYAML library in versions before 5.4, where it is susceptible to arbitrary code execution when it processes untrusted YAML files through the full_load method or with the FullLoader loader. Applications that use the library to process untrusted input may be vulnerable to this flaw. This flaw allows an attacker to execute arbitrary code on the system by abusing the python/object/new constructor. This flaw is due to an incomplete fix for CVE-2020-1747.',\n",
       " 'Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files). This vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings. It is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. PyPI packages are already available, and we hope that conda-forge packages will be available soon. If it is not possible to upgrade, we provide a separate package `pyarrow-hotfix` that disables the vulnerability on older PyArrow versions. See https://pypi.org/project/pyarrow-hotfix/ for instructions.',\n",
       " 'Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files). This vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings. It is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. PyPI packages are already available, and we hope that conda-forge packages will be available soon. If it is not possible to upgrade, we provide a separate package `pyarrow-hotfix` that disables the vulnerability on older PyArrow versions. See https://pypi.org/project/pyarrow-hotfix/ for instructions.',\n",
       " 'c-ares is an asynchronous resolver library. c-ares is vulnerable to denial of service. If a target resolver sends a query, the attacker forges a malformed UDP packet with a length of 0 and returns them to the target resolver. The target resolver erroneously interprets the 0 length as a graceful shutdown of the connection. This issue has been patched in version 1.19.1.',\n",
       " 'libuv is a multi-platform support library with a focus on asynchronous I/O. The `uv_getaddrinfo` function in `src/unix/getaddrinfo.c` (and its windows counterpart `src/win/getaddrinfo.c`), truncates hostnames to 256 characters before calling `getaddrinfo`. This behavior can be exploited to create addresses like `0x00007f000001`, which are considered valid by `getaddrinfo` and could allow an attacker to craft payloads that resolve to unintended IP addresses, bypassing developer checks. The vulnerability arises due to how the `hostname_ascii` variable (with a length of 256 bytes) is handled in `uv_getaddrinfo` and subsequently in `uv__idna_toascii`. When the hostname exceeds 256 characters, it gets truncated without a terminating null byte. As a result attackers may be able to access internal APIs or for websites (similar to MySpace) that allows users to have `username.example.com` pages. Internal services that crawl or cache these user pages can be exposed to SSRF attacks if a malicious user chooses a long vulnerable username. This issue has been addressed in release version 1.48.0. Users are advised to upgrade. There are no known workarounds for this vulnerability.',\n",
       " 'The llhttp parser in the http module in Node v20.2.0 does not strictly use the CRLF sequence to delimit HTTP requests. This can lead to HTTP Request Smuggling (HRS). The CR character (without LF) is sufficient to delimit HTTP header fields in the llhttp parser. According to RFC7230 section 3, only the CRLF sequence should delimit each header-field. This impacts all Node.js active versions: v16, v18, and, v20',\n",
       " 'In nghttp2 before version 1.41.0, the overly large HTTP/2 SETTINGS frame payload causes denial of service. The proof of concept attack involves a malicious client constructing a SETTINGS frame with a length of 14,400 bytes (2400 individual settings entries) over and over again. The attack causes the CPU to spike at 100%. nghttp2 v1.41.0 fixes this vulnerability. There is a workaround to this vulnerability. Implement nghttp2_on_frame_recv_callback callback, and if received frame is SETTINGS frame and the number of settings entries are large (e.g., > 32), then drop the connection.',\n",
       " 'The openssl (aka node-openssl) NPM package through 2.0.0 was characterized as \"a nonsense wrapper with no real purpose\" by its author, and accepts an opts argument that contains a verb field (used for command execution). NOTE: This vulnerability only affects products that are no longer supported by the maintainer.',\n",
       " 'A Spring Data MongoDB application is vulnerable to SpEL Injection when using @Query or @Aggregation-annotated query methods with SpEL expressions that contain query parameter placeholders for value binding if the input is not sanitized.',\n",
       " 'In Spring Framework versions 6.0.15 and 6.1.2, it is possible for a user to provide specially crafted HTTP requests that may cause a denial-of-service (DoS) condition. Specifically, an application is vulnerable when all of the following are true: * the application uses Spring MVC * Spring Security 6.1.6+ or 6.2.1+ is on the classpath Typically, Spring Boot applications need the org.springframework.boot:spring-boot-starter-web and org.springframework.boot:spring-boot-starter-security dependencies to meet all conditions.',\n",
       " 'jsonwebtoken is an implementation of JSON Web Tokens. Versions `<= 8.5.1` of `jsonwebtoken` library can be misconfigured so that passing a poorly implemented key retrieval function referring to the `secretOrPublicKey` argument from the readme link will result in incorrect verification of tokens. There is a possibility of using a different algorithm and key combination in verification, other than the one that was used to sign the tokens. Specifically, tokens signed with an asymmetric public key could be verified with a symmetric HS256 algorithm. This can lead to successful validation of forged tokens. If your application is supporting usage of both symmetric key and asymmetric key in jwt.verify() implementation with the same key retrieval function. This issue has been patched, please update to version 9.0.0.',\n",
       " 'H2 Console before 2.1.210 allows remote attackers to execute arbitrary code via a jdbc:h2:mem JDBC URL containing the IGNORE_UNKNOWN_SETTINGS=TRUE;FORBID_CREATION=FALSE;INIT=RUNSCRIPT substring, a different vulnerability than CVE-2021-42392.',\n",
       " 'The org.h2.util.JdbcUtils.getConnection method of the H2 database takes as parameters the class name of the driver and URL of the database. An attacker may pass a JNDI driver name and a URL leading to a LDAP or RMI servers, causing remote code execution. This can be exploited through various attack vectors, most notably through the H2 Console which leads to unauthenticated remote code execution.',\n",
       " 'A heap use-after-free issue has been identified in SQLite in the jsonParseAddNodeArray() function in sqlite3.c. This flaw allows a local attacker to leverage a victim to pass specially crafted malicious input to the application, potentially causing a crash and leading to a denial of service.',\n",
       " 'SQLite JDBC is a library for accessing and creating SQLite database files in Java. Sqlite-jdbc addresses a remote code execution vulnerability via JDBC URL. This issue impacting versions 3.6.14.1 through 3.41.2.1 and has been fixed in version 3.41.2.2.',\n",
       " 'The -ftrapv compiler option in gcc and g++ 3.3.3 and earlier does not handle all types of integer overflows, which may leave applications vulnerable to vulnerabilities related to overflows.',\n",
       " 'The issue was addressed with improved checks. This issue is fixed in Xcode 14.0. Parsing a file may lead to disclosure of user information.',\n",
       " 'An injection issue was addressed with improved input validation. This issue is fixed in Xcode 14.1. An app may be able to gain root privileges.',\n",
       " 'Jenkins Xcode integration Plugin 2.0.14 and earlier does not configure its XML parser to prevent XML external entity (XXE) attacks.',\n",
       " 'Use after free in libxml2 before 2.9.5, as used in Google Chrome prior to 63.0.3239.84 and other products, allowed a remote attacker to potentially exploit heap corruption via a crafted HTML page.',\n",
       " 'libxml2, as used in Red Hat JBoss Core Services and when in recovery mode, allows context-dependent attackers to cause a denial of service (stack consumption) via a crafted XML document. NOTE: this vulnerability exists because of an incorrect fix for CVE-2016-3627.',\n",
       " 'cmake installs the cmake x86 linux binaries. cmake downloads binary resources over HTTP, which leaves it vulnerable to MITM attacks. It may be possible to cause remote code execution (RCE) by swapping out the requested binary with an attacker controlled binary if the attacker is on the network or positioned in between the user and the remote server.',\n",
       " 'Untrusted search path vulnerability in CMake before 2.2.0-r1 on Gentoo Linux allows local users in the portage group to gain privileges via a malicious shared object in the Portage temporary build directory, which is part of the RUNPATH.',\n",
       " 'loadAsync in JSZip before 3.8.0 allows Directory Traversal via a crafted ZIP archive.',\n",
       " 'This affects the package jszip before 3.7.0. Crafting a new zip file with filenames set to Object prototype values (e.g __proto__, toString, etc) results in a returned object with a modified prototype instance.',\n",
       " 'WebTorrent before 0.107.6 allows XSS in the HTTP server via a title or file name.',\n",
       " 'JSON5 is an extension to the popular JSON file format that aims to be easier to write and maintain by hand (e.g. for config files). The `parse` method of the JSON5 library before and including versions 1.0.1 and 2.2.1 does not restrict parsing of keys named `__proto__`, allowing specially crafted strings to pollute the prototype of the resulting object. This vulnerability pollutes the prototype of the object returned by `JSON5.parse` and not the global Object prototype, which is the commonly understood definition of Prototype Pollution. However, polluting the prototype of a single object can have significant security impact for an application if the object is later used in trusted operations. This vulnerability could allow an attacker to set arbitrary and unexpected keys on the object returned from `JSON5.parse`. The actual impact will depend on how applications utilize the returned object and how they filter unwanted keys, but could include denial of service, cross-site scripting, elevation of privilege, and in extreme cases, remote code execution. `JSON5.parse` should restrict parsing of `__proto__` keys when parsing JSON strings to objects. As a point of reference, the `JSON.parse` method included in JavaScript ignores `__proto__` keys. Simply changing `JSON5.parse` to `JSON.parse` in the examples above mitigates this vulnerability. This vulnerability is patched in json5 versions 1.0.2, 2.2.2, and later.',\n",
       " 'Prototype pollution vulnerability in function parseQuery in parseQuery.js in webpack loader-utils via the name variable in parseQuery.js. This affects all versions prior to 1.4.1 and 2.0.3.',\n",
       " 'The package terser before 4.8.1, from 5.0.0 and before 5.14.2 are vulnerable to Regular Expression Denial of Service (ReDoS) due to insecure usage of regular expressions.',\n",
       " 'Auth. (admin+) Stored Cross-Site Scripting (XSS) vulnerability in iControlWP Article Directory Redux plugin <= 1.0.2 versions.',\n",
       " 'Auth. (admin+) Stored Cross-Site Scripting (XSS) vulnerability in iControlWP Article Directory Redux plugin <= 1.0.2 versions.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_knowledge = list(eval_df['Local_knowledge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(local_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chatbot Wiki Document:\\n\\nThe application\\'s database is structured as a vector database, specifically using Chroma DB, and document embeddings are generated using LangChain and OpenAI\\'s \"text_ada_002.\"\\n\\n3. Database Structure\\n- The database is organized in the form of a vector database using Chroma DB.\\n- Document embeddings are generated using LangChain and OpenAI\\'s \"text_ada_002\" to represent text documents as vectors.\\n\\n3. Document Embeddings\\n- Document embeddings are created using LangChain and OpenAI\\'s \"text_ada_002\" to represent text documents\\nas vectors.\\n\\nDependencies\\nChatBot relies on the following Python libraries and versions:\\n\\nLangchain == 0.0.194\\n\\nLangChain is a comprehensive framework designed for the development of applications powered by language models. It facilitates the creation of context-aware applications that can connect language models to various sources of context, such as prompt instructions, few-shot examples, or relevant content, enabling them to provide more accurate and relevant responses. Additionally, LangChain enables applications to reason by relying on language models to determine how to respond based on the provided context and what actions to take. The framework comprises several components, including LangChain Libraries, which consist of Python and JavaScript libraries containing interfaces, integrations for various components, a basic runtime for combining these components into chains and agents, and pre-built implementations of chains and agents. Furthermore, LangChain offers LangChain Templates, a collection of easily deployable reference architectures for a wide range of tasks, LangServe, a library for deploying LangChain chains as a REST API, and LangSmith, a developer platform that facilitates debugging, testing, evaluation, and monitoring of chains built on any LLM framework while seamlessly integrating with LangChain.\\n\\nFor our ChatBot application we utilize LangChain framework. We use Langchain version 0.0.194 for our application. It provides the us platform to adapt language model with flexibility to our specific context. We utilize some of the core components within the langchain to build context-aware language models systems. To achieve this, we use core components including LLM interface, chains, prompt templates, retrieval modules, memory. In case of LLM interface, we interface with proprietary models such as GPT to make API calls. We leverage gpt-3.5-turbo as our base generator model. Chains are the fundamental principle that holds various AI components in LangChain to provide context-aware responses. A chain is a series of automated actions from the user\\'s query to the model\\'s output. Chain in our case implements Program-Aided Language Models (PAL) for generating code solutions. This is utilized to answer math or code related questions in our interface. PALChain reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.',\n",
       " 'Chatbot Wiki Document:\\n\\nThe application\\'s database is structured as a vector database, specifically using Chroma DB, and document embeddings are generated using LangChain and OpenAI\\'s \"text_ada_002.\"\\n\\n3. Database Structure\\n- The database is organized in the form of a vector database using Chroma DB.\\n- Document embeddings are generated using LangChain and OpenAI\\'s \"text_ada_002\" to represent text documents as vectors.\\n\\n3. Document Embeddings\\n- Document embeddings are created using LangChain and OpenAI\\'s \"text_ada_002\" to represent text documents\\nas vectors.\\n\\nDependencies\\nChatBot relies on the following Python libraries and versions:\\n\\nLangchain == 0.0.194\\n\\nLangChain is a comprehensive framework designed for the development of applications powered by language models. It facilitates the creation of context-aware applications that can connect language models to various sources of context, such as prompt instructions, few-shot examples, or relevant content, enabling them to provide more accurate and relevant responses. Additionally, LangChain enables applications to reason by relying on language models to determine how to respond based on the provided context and what actions to take. The framework comprises several components, including LangChain Libraries, which consist of Python and JavaScript libraries containing interfaces, integrations for various components, a basic runtime for combining these components into chains and agents, and pre-built implementations of chains and agents. Furthermore, LangChain offers LangChain Templates, a collection of easily deployable reference architectures for a wide range of tasks, LangServe, a library for deploying LangChain chains as a REST API, and LangSmith, a developer platform that facilitates debugging, testing, evaluation, and monitoring of chains built on any LLM framework while seamlessly integrating with LangChain.\\n\\nFor our ChatBot application we utilize LangChain framework. We use Langchain version 0.0.194 for our application. It provides the us platform to adapt language model with flexibility to our specific context. We utilize some of the core components within the langchain to build context-aware language models systems. To achieve this, we use core components including LLM interface, chains, prompt templates, retrieval modules, memory. In case of LLM interface, we interface with proprietary models such as GPT to make API calls. We leverage gpt-3.5-turbo as our base generator model. Chains are the fundamental principle that holds various AI components in LangChain to provide context-aware responses. A chain is a series of automated actions from the user\\'s query to the model\\'s output. Chain in our case implements Program-Aided Language Models (PAL) for generating code solutions. This is utilized to answer math or code related questions in our interface. PALChain reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.',\n",
       " 'Chatbot Wiki Document:\\n\\nStreamlit == 0.63.0\\n\\nStreamlit is a free and open-source framework to rapidly build and share beautiful machine learning and data science web apps. It is a Python-based library specifically designed for machine learning engineers. Streamlit allows you to create a stunning-looking application with only a few lines of code.In our chatbot application we utilize streamlit version 0.63.0. To do so we first install streamlit using pip command. The syntax for installation is pip install streamlit. We then create logic for our chatbot. his script will handle user inputs, process them, and generate appropriate responses.\\n\\nWe use Docker version 3.1 for ChatBot application. It is hosted in Linux environment (ubuntu22.04). The services we run on docker environment is streamlit. The application runs on port 8501. The resources for deployment in terms are as follows. For drivers we use NVidia, and count is set for all, and capabilities is set to GPU. In relation to nginx, the image is nginx and volumes are - ./nginx/streamlit.conf:/etc/nginx/conf.d/default.conf- /etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem- /etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem. The port for nginx is 4000:443',\n",
       " 'Chatbot Wiki Document:\\n\\nStreamlit == 0.63.0\\n\\nStreamlit is a free and open-source framework to rapidly build and share beautiful machine learning and data science web apps. It is a Python-based library specifically designed for machine learning engineers. Streamlit allows you to create a stunning-looking application with only a few lines of code.In our chatbot application we utilize streamlit version 0.63.0. To do so we first install streamlit using pip command. The syntax for installation is pip install streamlit. We then create logic for our chatbot. his script will handle user inputs, process them, and generate appropriate responses.\\n\\nWe use Docker version 3.1 for ChatBot application. It is hosted in Linux environment (ubuntu22.04). The services we run on docker environment is streamlit. The application runs on port 8501. The resources for deployment in terms are as follows. For drivers we use NVidia, and count is set for all, and capabilities is set to GPU. In relation to nginx, the image is nginx and volumes are - ./nginx/streamlit.conf:/etc/nginx/conf.d/default.conf- /etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem- /etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem. The port for nginx is 4000:443',\n",
       " 'Chatbot Wiki Document:\\n\\nDocker == 3.1\\n\\nDocker Desktop is a one-click-install application for your Mac, Linux, or Windows environment that lets you build, share, and run containerized applications and microservices.\\nWe use Docker version 3.1 for ChatBot application. It is hosted in Linux environment (ubuntu22.04). The services we run on docker environment is streamlit. The application runs on port 8501. The resources for deployment in terms are as follows. For drivers we use NVidia, and count is set for all, and capabilities is set to GPU. In relation to nginx, the image is nginx and volumes are - ./nginx/streamlit.conf:/etc/nginx/conf.d/default.conf- /etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem- /etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem. The port for nginx is 4000:443\\n\\n\\nUI Wiki Document:\\n\\nThe FaceApp web application, designed for social media interactions, utilizes a Node.js for frontend and has no backend. The entire application is containerized using Docker and hosted on AWS. The deployment process\\nincorporates continuous integration and continuous deployment (CI/CD) practices to ensure a smooth and automated workflow.\\n\\nDeployment\\nDocker: The application is containerized using Docker, ensuring consistency and portability across different environments.\\n\\nThe application is containerized using Docker, ensuring consistency and ease of deployment.\\nDeployment to AWS:\\nThe Dockerized application is deployed to the AWS infrastructure, leveraging cloud services for hosting.\\n\\nArtifact Generation: Creates a Docker image containing the application and its dependencies.\\nArtifact Storage: Stores the Docker image in a container registry (e.g., Amazon ECR).\\n\\n3. Continuous Deployment (CD)\\nThe CD pipeline deploys the Dockerized FaceApp application to the AWS infrastructure.\\nCD Workflow:\\nArtifact Retrieval: Pulls the Docker image from the container registry.\\nInfrastructure Provisioning: Creates or updates the necessary AWS resources (e.g., EC2 instances, Load Balancers)\\nusing Infrastructure as Code (IaC) tools like AWS CloudFormation.\\nContainer Deployment: Deploys the Docker image to the provisioned infrastructure.\\n\\nDOCKER_REGISTRY_URL: URL of the Docker registry (e.g., ECR) for image storage.\\nECR_REPOSITORY: Name of the repository in the Docker registry.\\n\\n\\nBackend API Wiki Document:\\n\\n3. Containerization\\nThe deployable artifact (JAR file) is containerized using Docker for consistent deployment across different\\nenvironments.\\n\\nContainerization Steps:\\nDocker Image Build: Constructs a Docker image containing the Spring Boot application and its dependencies.\\nImage Tagging: Tags the Docker image with a version or unique identifier.\\nPush to Registry: Pushes the Docker image to a container registry (e.g., Docker Hub, Amazon ECR).\\n4. Continuous Deployment (CD)\\nThe CD pipeline automates the deployment and configuration of the ResumeLink backend on target environments.\\nCD Workflow:\\nArtifact Retrieval: Pulls the Docker image from the container registry.\\nEnvironment Provisioning: Sets up the target environment, including databases and other required services.\\nConfiguration Management: Applies environment-specific configurations (e.g., database connections, security\\nsettings).\\nContainer Deployment: Deploys the Docker image on the target environment.\\nSmoke Testing: Performs basic tests to ensure the deployed application is functioning.\\nSecurity Token Configuration: Configures and manages JWT secret keys for secure communication.',\n",
       " 'Chatbot Wiki Document:\\n\\nDocker == 3.1\\n\\nDocker Desktop is a one-click-install application for your Mac, Linux, or Windows environment that lets you build, share, and run containerized applications and microservices.\\nWe use Docker version 3.1 for ChatBot application. It is hosted in Linux environment (ubuntu22.04). The services we run on docker environment is streamlit. The application runs on port 8501. The resources for deployment in terms are as follows. For drivers we use NVidia, and count is set for all, and capabilities is set to GPU. In relation to nginx, the image is nginx and volumes are - ./nginx/streamlit.conf:/etc/nginx/conf.d/default.conf- /etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/privkey.pem- /etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem:/etc/letsencrypt/live/patentlab.cse.msstate.edu/cert.pem. The port for nginx is 4000:443\\n\\n\\nUI Wiki Document:\\n\\nThe FaceApp web application, designed for social media interactions, utilizes a Node.js for frontend and has no backend. The entire application is containerized using Docker and hosted on AWS. The deployment process\\nincorporates continuous integration and continuous deployment (CI/CD) practices to ensure a smooth and automated workflow.\\n\\nDeployment\\nDocker: The application is containerized using Docker, ensuring consistency and portability across different environments.\\n\\nThe application is containerized using Docker, ensuring consistency and ease of deployment.\\nDeployment to AWS:\\nThe Dockerized application is deployed to the AWS infrastructure, leveraging cloud services for hosting.\\n\\nArtifact Generation: Creates a Docker image containing the application and its dependencies.\\nArtifact Storage: Stores the Docker image in a container registry (e.g., Amazon ECR).\\n\\n3. Continuous Deployment (CD)\\nThe CD pipeline deploys the Dockerized FaceApp application to the AWS infrastructure.\\nCD Workflow:\\nArtifact Retrieval: Pulls the Docker image from the container registry.\\nInfrastructure Provisioning: Creates or updates the necessary AWS resources (e.g., EC2 instances, Load Balancers)\\nusing Infrastructure as Code (IaC) tools like AWS CloudFormation.\\nContainer Deployment: Deploys the Docker image to the provisioned infrastructure.\\n\\nDOCKER_REGISTRY_URL: URL of the Docker registry (e.g., ECR) for image storage.\\nECR_REPOSITORY: Name of the repository in the Docker registry.\\n\\n\\nBackend API Wiki Document:\\n\\n3. Containerization\\nThe deployable artifact (JAR file) is containerized using Docker for consistent deployment across different\\nenvironments.\\n\\nContainerization Steps:\\nDocker Image Build: Constructs a Docker image containing the Spring Boot application and its dependencies.\\nImage Tagging: Tags the Docker image with a version or unique identifier.\\nPush to Registry: Pushes the Docker image to a container registry (e.g., Docker Hub, Amazon ECR).\\n4. Continuous Deployment (CD)\\nThe CD pipeline automates the deployment and configuration of the ResumeLink backend on target environments.\\nCD Workflow:\\nArtifact Retrieval: Pulls the Docker image from the container registry.\\nEnvironment Provisioning: Sets up the target environment, including databases and other required services.\\nConfiguration Management: Applies environment-specific configurations (e.g., database connections, security\\nsettings).\\nContainer Deployment: Deploys the Docker image on the target environment.\\nSmoke Testing: Performs basic tests to ensure the deployed application is functioning.\\nSecurity Token Configuration: Configures and manages JWT secret keys for secure communication.',\n",
       " 'Chatbot Wiki Document:\\n\\nGitpython == 3.1.40\\nGitPython is a python library used to interact with Git repositories. GitPython provides object model read and write access to your git repository. Access repository information conveniently, alter the index directly, handle remotes, or go down to low-level object database access with big-files support.\\nFor our chatbot application, GitPython version 3.1.40 is used in various ways, depending on our specific\\nrequirements. We particularly use it for version control for chatbot code which include tasks such as cloning a repository containing chatbot code, pulling updates from remote repository, committing changes made to the\\ncodebase, and pushing those changes back to the remote repository.',\n",
       " 'Chatbot Wiki Document:\\n\\nGitpython == 3.1.40\\nGitPython is a python library used to interact with Git repositories. GitPython provides object model read and write access to your git repository. Access repository information conveniently, alter the index directly, handle remotes, or go down to low-level object database access with big-files support.\\nFor our chatbot application, GitPython version 3.1.40 is used in various ways, depending on our specific\\nrequirements. We particularly use it for version control for chatbot code which include tasks such as cloning a repository containing chatbot code, pulling updates from remote repository, committing changes made to the\\ncodebase, and pushing those changes back to the remote repository.',\n",
       " \"Chatbot Wiki Document:\\n\\nJobLib == 1.0.0\\n\\nJoblib offers a suite of tools aimed at facilitating lightweight pipelining in Python. Its primary features include transparent disk-caching of functions, enabling lazy re-evaluation through the memoize pattern, and providing straightforward support for parallel computing. It's designed with optimization for speed and robustness, especially when handling large datasets, with specific enhancements tailored for NumPy arrays. This makes Joblib particularly useful for efficiently caching function outputs, performing parallel computations, and managing resources in Python applications, particularly those involving data-intensive tasks.\\nIn our chatbot application we use joblib version 1.0.0. We use it for model caching, for example, we cache the results of user inputs that utilizes the LLM model to avoid re-running it for every user query. Similarly, we also use Joblib for resource management task such as large dataset or pretrained models. By efficiently serializing and deserializing these resources, Joblib can reduce memory usage and improve performance.\",\n",
       " \"Chatbot Wiki Document:\\n\\nJobLib == 1.0.0\\n\\nJoblib offers a suite of tools aimed at facilitating lightweight pipelining in Python. Its primary features include transparent disk-caching of functions, enabling lazy re-evaluation through the memoize pattern, and providing straightforward support for parallel computing. It's designed with optimization for speed and robustness, especially when handling large datasets, with specific enhancements tailored for NumPy arrays. This makes Joblib particularly useful for efficiently caching function outputs, performing parallel computations, and managing resources in Python applications, particularly those involving data-intensive tasks.\\nIn our chatbot application we use joblib version 1.0.0. We use it for model caching, for example, we cache the results of user inputs that utilizes the LLM model to avoid re-running it for every user query. Similarly, we also use Joblib for resource management task such as large dataset or pretrained models. By efficiently serializing and deserializing these resources, Joblib can reduce memory usage and improve performance.\",\n",
       " 'Chatbot Wiki Document:\\n\\nmarkdown-it-py == 2.1.0\\n\\nMarkdown-it is a robust, extensible, fast, and easy-to-use markdown parser for Node.js and the browser. This library follows the CommonMark specification and provides a variety of syntax extensions and functionality for processing markdown text. With Markdown-it, you can generate HTML or render tokens sequence from your markdown text. It also allows you to replace the existing syntax rules, add new ones, and includes a variety of plugins for extending the libraryâ€™s functionality.\\nFor our Chatbot application we use Markdown-it version 2.1.0 to generate HTML. We do this via API call. The\\npackage we use is markdown-it-py package. The transformation take place by first converting raw text into tokens and then converting to other formats using renderers.',\n",
       " 'Chatbot Wiki Document:\\n\\nmarkdown-it-py == 2.1.0\\n\\nMarkdown-it is a robust, extensible, fast, and easy-to-use markdown parser for Node.js and the browser. This library follows the CommonMark specification and provides a variety of syntax extensions and functionality for processing markdown text. With Markdown-it, you can generate HTML or render tokens sequence from your markdown text. It also allows you to replace the existing syntax rules, add new ones, and includes a variety of plugins for extending the libraryâ€™s functionality.\\nFor our Chatbot application we use Markdown-it version 2.1.0 to generate HTML. We do this via API call. The\\npackage we use is markdown-it-py package. The transformation take place by first converting raw text into tokens and then converting to other formats using renderers.',\n",
       " 'Chatbot Wiki Document:\\n\\ntoml = = 2.14.0\\n\\nTOML, short for \"Tom\\'s Obvious, Minimal Language,\" is a human-readable data serialization format. It aims to be easy to read and write due to its straightforward syntax, while also being unambiguous for parsing. TOML is commonly used for configuration files, particularly in projects written in programming languages such as Rust, Python, and Go, among others.TOML syntax is based on key-value pairs, with support for various data types like strings, integers, floating-point numbers, arrays, and nested tables. It organizes data in a hierarchical structure, making it suitable for representing complex configurations.\\n\\nIn our chatbot application, TOML version 2.14.0 is primarily used for configuration purposes. It helps organize settings such as API keys, language resources, intent mappings, and plugin configurations. TOML\\'s hierarchical structure enables clear organization of settings, facilitating easier maintenance and customization of the chatbot\\'s behavior. For example, a sample configuration for our Chatbot application using TOML is as follows: api_key = \"YOUR_API_KEY\" language = \"en\" max_responses = 5',\n",
       " 'Chatbot Wiki Document:\\n\\ntoml = = 2.14.0\\n\\nTOML, short for \"Tom\\'s Obvious, Minimal Language,\" is a human-readable data serialization format. It aims to be easy to read and write due to its straightforward syntax, while also being unambiguous for parsing. TOML is commonly used for configuration files, particularly in projects written in programming languages such as Rust, Python, and Go, among others.TOML syntax is based on key-value pairs, with support for various data types like strings, integers, floating-point numbers, arrays, and nested tables. It organizes data in a hierarchical structure, making it suitable for representing complex configurations.\\n\\nIn our chatbot application, TOML version 2.14.0 is primarily used for configuration purposes. It helps organize settings such as API keys, language resources, intent mappings, and plugin configurations. TOML\\'s hierarchical structure enables clear organization of settings, facilitating easier maintenance and customization of the chatbot\\'s behavior. For example, a sample configuration for our Chatbot application using TOML is as follows: api_key = \"YOUR_API_KEY\" language = \"en\" max_responses = 5',\n",
       " 'Chatbot Wiki Document:\\n\\ntqdm == 4.4.1\\n\\nTQDM stands for \"TQDM Quick Documentation Markup.\" It is a Python library that provides a fast, extensible progress bar for loops and iterables. TQDM stands for \"Taichi QUick Documentation Markup.\" TQDM provides a simple way to add progress bars to your loops in Python, allowing you to track the progress of your code\\'s execution. It\\'s particularly useful when working with long-running tasks or iterating over large datasets, as it provides real-time feedback on the progress of your code.\\n\\nOur chatbot application involves heavy data processing tasks such as data loading, preprocessing, or model training, for these task we use TQDM version 4.4.1 to display progress bars for these tasks. This provides real-time feedback to users on the progress of the processing.',\n",
       " 'Chatbot Wiki Document:\\n\\ntqdm == 4.4.1\\n\\nTQDM stands for \"TQDM Quick Documentation Markup.\" It is a Python library that provides a fast, extensible progress bar for loops and iterables. TQDM stands for \"Taichi QUick Documentation Markup.\" TQDM provides a simple way to add progress bars to your loops in Python, allowing you to track the progress of your code\\'s execution. It\\'s particularly useful when working with long-running tasks or iterating over large datasets, as it provides real-time feedback on the progress of your code.\\n\\nOur chatbot application involves heavy data processing tasks such as data loading, preprocessing, or model training, for these task we use TQDM version 4.4.1 to display progress bars for these tasks. This provides real-time feedback to users on the progress of the processing.',\n",
       " \"Chatbot Wiki Document:\\n\\nNLTK == 3.6.2\\n\\nNLTK stands for Natural Language Toolkit, which is a comprehensive library in Python used for natural language processing (NLP) tasks. It provides tools and resources for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and semantic reasoning.\\n\\nIn our chatbot application, NLTK is utilized for various purposes such as Text Processing and Language\\nUnderstanding. We particularly utilize NLTK version 3.6.2. For text processing task, NLTK version 3.6.2\\npreprocess and clean the text input from users, including tokenization (breaking text into words or sentences), stemming (reducing words to their root forms), and lemmatization (reducing words to their base or dictionary forms). While for language understanding task, NLTK version 3.6.2 is used to analyze the user's input to understandthe intent and extract relevant information. This includes part-of-speech tagging to identify the grammatical structure of sentences, named entity recognition to identify entities like names, dates, and locations, and sentiment analysis to determine the sentiment expressed in the text.\",\n",
       " \"Chatbot Wiki Document:\\n\\nNLTK == 3.6.2\\n\\nNLTK stands for Natural Language Toolkit, which is a comprehensive library in Python used for natural language processing (NLP) tasks. It provides tools and resources for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and semantic reasoning.\\n\\nIn our chatbot application, NLTK is utilized for various purposes such as Text Processing and Language\\nUnderstanding. We particularly utilize NLTK version 3.6.2. For text processing task, NLTK version 3.6.2\\npreprocess and clean the text input from users, including tokenization (breaking text into words or sentences), stemming (reducing words to their root forms), and lemmatization (reducing words to their base or dictionary forms). While for language understanding task, NLTK version 3.6.2 is used to analyze the user's input to understandthe intent and extract relevant information. This includes part-of-speech tagging to identify the grammatical structure of sentences, named entity recognition to identify entities like names, dates, and locations, and sentiment analysis to determine the sentiment expressed in the text.\",\n",
       " 'Chatbot Wiki Document:\\n\\nPandas == 1.4.3\\n\\nPandas is a Python library primarily used for data manipulation and analysis. It provides data structures and\\nfunctions to efficiently handle structured data, such as tables and time series, making it particularly\\nuseful for tasks like data cleaning, transformation, exploration, and visualization.\\n\\nIn the context of our chatbot application, we use Pandas version 1.4.3 for data processing. Pandas is utilized to preprocess and clean datasets before training machine learning models for the chatbot. This includes tasks such as removing duplicates, handling missing values, and transforming data into a format suitable for analysis.\\n\\nPyArrow == 13.0.0\\n\\nPyArrow is a Python library designed for working with large datasets efficiently, particularly in the context of data serialization and inter process communication. It provides functionalities for converting between different data formats, such as Pandas DataFrames and Apache Parquet files, with high performance and low memory overhead. \\n\\nIn the context of a chatbot application, PyArrow version 13.0.0 is utilized for Integration with Pandas. PyArrow seamlessly integrates with Pandas, a popular data manipulation library in Python. This allows the chatbot application to process large datasets efficiently, perform computations, and generate insights using Pandas DataFrames.',\n",
       " 'Chatbot Wiki Document:\\n\\nPandas == 1.4.3\\n\\nPandas is a Python library primarily used for data manipulation and analysis. It provides data structures and\\nfunctions to efficiently handle structured data, such as tables and time series, making it particularly\\nuseful for tasks like data cleaning, transformation, exploration, and visualization.\\n\\nIn the context of our chatbot application, we use Pandas version 1.4.3 for data processing. Pandas is utilized to preprocess and clean datasets before training machine learning models for the chatbot. This includes tasks such as removing duplicates, handling missing values, and transforming data into a format suitable for analysis.\\n\\nPyArrow == 13.0.0\\n\\nPyArrow is a Python library designed for working with large datasets efficiently, particularly in the context of data serialization and inter process communication. It provides functionalities for converting between different data formats, such as Pandas DataFrames and Apache Parquet files, with high performance and low memory overhead. \\n\\nIn the context of a chatbot application, PyArrow version 13.0.0 is utilized for Integration with Pandas. PyArrow seamlessly integrates with Pandas, a popular data manipulation library in Python. This allows the chatbot application to process large datasets efficiently, perform computations, and generate insights using Pandas DataFrames.',\n",
       " \"Chatbot Wiki Document:\\n\\nScipy == 0.9.0\\n\\nScipy is a Python library used for scientific computing and technical computing. It builds on top of NumPy,\\nproviding additional functionality for tasks such as optimization, integration, interpolation, linear algebra, signal processing, and statistical analysis.\\n\\nIn the context of our chatbot application, Scipy version 0.9.0 is be utilized for Statistical analysis. For Statistical analysis we use functions for computing descriptive statistics, hypothesis testing, probability distributions, and correlation analysis. We do this to analyze user data collected by the chatbot and derive insights from it. Our chatbot collects user feedback through a rating system, and we want to analyze the distribution of ratings to understand user satisfaction. In such case we use Scipy's statistical functions to calculate descriptive statistics like the mean, median, and standard deviation of the ratings. Additionally, we perform hypothesis tests to compare the ratings between different user groups or time periods. This analysis can help us to identify areas for improvement and make data-driven decisions to enhance user experience.\",\n",
       " \"Chatbot Wiki Document:\\n\\nScipy == 0.9.0\\n\\nScipy is a Python library used for scientific computing and technical computing. It builds on top of NumPy,\\nproviding additional functionality for tasks such as optimization, integration, interpolation, linear algebra, signal processing, and statistical analysis.\\n\\nIn the context of our chatbot application, Scipy version 0.9.0 is be utilized for Statistical analysis. For Statistical analysis we use functions for computing descriptive statistics, hypothesis testing, probability distributions, and correlation analysis. We do this to analyze user data collected by the chatbot and derive insights from it. Our chatbot collects user feedback through a rating system, and we want to analyze the distribution of ratings to understand user satisfaction. In such case we use Scipy's statistical functions to calculate descriptive statistics like the mean, median, and standard deviation of the ratings. Additionally, we perform hypothesis tests to compare the ratings between different user groups or time periods. This analysis can help us to identify areas for improvement and make data-driven decisions to enhance user experience.\",\n",
       " \"Chatbot Wiki Document:\\n\\nJobLib == 1.0.0\\n\\nJoblib offers a suite of tools aimed at facilitating lightweight pipelining in Python. Its primary features include transparent disk-caching of functions, enabling lazy re-evaluation through the memoize pattern, and providing straightforward support for parallel computing. It's designed with optimization for speed and robustness, especially when handling large datasets, with specific enhancements tailored for NumPy arrays. This makes Joblib particularly useful for efficiently caching function outputs, performing parallel computations, and managing resources in Python applications, particularly those involving data-intensive tasks.\\n\\nScipy == 0.9.0\\n\\nScipy is a Python library used for scientific computing and technical computing. It builds on top of NumPy, providing additional functionality for tasks such as optimization, integration, interpolation, linear algebra, signal processing, and statistical analysis.\\n\\nNumpy == 1.15.0\\n\\nNumPy is a powerful Python library for numerical computing that provides support for large, multi-dimensional\\narrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. It is widely used in scientific and technical computing tasks, including data manipulation, linear algebra, statistical analysis, and more. \\n\\nIn the context of our chatbot application, NumPy version 1.15.0 is leveraged for various purposes, such as: Data preprocessing: NumPy arrays can be utilized to represent and manipulate textual or numerical data collected by the chatbot. For example, user inputs or feedback ratings can be stored in NumPy arrays for further processing.\",\n",
       " \"Chatbot Wiki Document:\\n\\nJobLib == 1.0.0\\n\\nJoblib offers a suite of tools aimed at facilitating lightweight pipelining in Python. Its primary features include transparent disk-caching of functions, enabling lazy re-evaluation through the memoize pattern, and providing straightforward support for parallel computing. It's designed with optimization for speed and robustness, especially when handling large datasets, with specific enhancements tailored for NumPy arrays. This makes Joblib particularly useful for efficiently caching function outputs, performing parallel computations, and managing resources in Python applications, particularly those involving data-intensive tasks.\\n\\nScipy == 0.9.0\\n\\nScipy is a Python library used for scientific computing and technical computing. It builds on top of NumPy, providing additional functionality for tasks such as optimization, integration, interpolation, linear algebra, signal processing, and statistical analysis.\\n\\nNumpy == 1.15.0\\n\\nNumPy is a powerful Python library for numerical computing that provides support for large, multi-dimensional\\narrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. It is widely used in scientific and technical computing tasks, including data manipulation, linear algebra, statistical analysis, and more. \\n\\nIn the context of our chatbot application, NumPy version 1.15.0 is leveraged for various purposes, such as: Data preprocessing: NumPy arrays can be utilized to represent and manipulate textual or numerical data collected by the chatbot. For example, user inputs or feedback ratings can be stored in NumPy arrays for further processing.\",\n",
       " 'Chatbot Wiki Document:\\n\\nPillow == 10.0.0\\n\\nPillow is a Python Imaging Library (PIL) fork that adds support for opening, manipulating, and saving many\\ndifferent image file formats.\\n\\nIn the context of a chatbot application, Pillow version 10.0.0 is used for various image processing tasks, such as resizing images for UI interface. We use 3 images. These images are for mascot, logo and banner in the interface.',\n",
       " 'Chatbot Wiki Document:\\n\\nPillow == 10.0.0\\n\\nPillow is a Python Imaging Library (PIL) fork that adds support for opening, manipulating, and saving many\\ndifferent image file formats.\\n\\nIn the context of a chatbot application, Pillow version 10.0.0 is used for various image processing tasks, such as resizing images for UI interface. We use 3 images. These images are for mascot, logo and banner in the interface.',\n",
       " \"Chatbot Wiki Document:\\n\\nPyYAML == 5.1\\n\\nPyYAML is a Python library that enables parsing and emitting YAML (YAML Ain't Markup Language) files. YAML is a human-readable data serialization format that is commonly used for configuration files, data exchange, and other structured data representation tasks. In a chatbot application, PyYAML version 5.1 is utilized for purposes, such as Configuration Management PyYAML is used to parse YAML configuration files that define various settings and parameters for the chatbot application. This includes defining conversation flows, setting up external API credentials, specifying user interface preferences, and more.\",\n",
       " \"Chatbot Wiki Document:\\n\\nPyYAML == 5.1\\n\\nPyYAML is a Python library that enables parsing and emitting YAML (YAML Ain't Markup Language) files. YAML is a human-readable data serialization format that is commonly used for configuration files, data exchange, and other structured data representation tasks. In a chatbot application, PyYAML version 5.1 is utilized for purposes, such as Configuration Management PyYAML is used to parse YAML configuration files that define various settings and parameters for the chatbot application. This includes defining conversation flows, setting up external API credentials, specifying user interface preferences, and more.\",\n",
       " 'Chatbot Wiki Document:\\n\\nPyArrow == 13.0.0\\n\\nPyArrow is a Python library designed for working with large datasets efficiently, particularly in the context of data serialization and inter process communication. It provides functionalities for converting between different data formats, such as Pandas DataFrames and Apache Parquet files, with high performance and low memory overhead.\\nIn the context of a chatbot application, PyArrow version 13.0.0 is utilized for Integration with Pandas. PyArrow seamlessly integrates with Pandas, a popular data manipulation library in Python. This allows the chatbot application to process large datasets efficiently, perform computations, and generate insights using Pandas DataFrames.',\n",
       " 'Chatbot Wiki Document:\\n\\nPyArrow == 13.0.0\\n\\nPyArrow is a Python library designed for working with large datasets efficiently, particularly in the context of data serialization and inter process communication. It provides functionalities for converting between different data formats, such as Pandas DataFrames and Apache Parquet files, with high performance and low memory overhead.\\nIn the context of a chatbot application, PyArrow version 13.0.0 is utilized for Integration with Pandas. PyArrow seamlessly integrates with Pandas, a popular data manipulation library in Python. This allows the chatbot application to process large datasets efficiently, perform computations, and generate insights using Pandas DataFrames.',\n",
       " 'UI Wiki Document:\\n\\nDependencies:\\n\\nc-ares == 1.19.0\\n\\nC-ares is a sub dependency within the NodeJs package.\\nOur faceapp application uses c-ares 1.19.0 NodeJsâ€™s sub dependency for asynchronous DNS requests. It is an asynchronous resolver library.',\n",
       " \"UI Wiki Document:\\n\\nLibuv == 1.47.0\\n\\nThe libuv dependency is a multi-platform support library with a focus on asynchronous I/O. It is primarily\\ndeveloped for use by Node.js.\\n\\nFor our faceapp application, we utilize libuv version 1.47.0 to abstract non-blocking I/O operations, which allows Node.js to achieve high performance and handle a large number of concurrent connections. This version also provides cross-platform support for features such as networking, threading, and operating system-related\\nfunctionality, making it an essential component of Node.js's event-driven architecture.\",\n",
       " 'UI Wiki Document:\\n\\nLlhttp == 16.19.0\\n\\nThe llhttp dependency is the http parser used by Node.js. LLHTTP (Low-Level HTTP) is a Node.js module that offers a lower-level interface for interacting with HTTP requests and responses.\\nIn our faceapp application, LLHTTP version 16.19.0 bypasses the higher-level abstractions provided by the built-in http module, giving us more granular control over the HTTP protocol. This is beneficial for specific use cases where fine-grained control is necessary, such as: Customizing HTTP headers and bodies, Handling raw data, and Performance optimization.',\n",
       " \"UI Wiki Document:\\n\\nNghttp2 == 1.40.0\\n\\nThe nghttp2 dependency is a C library implementing HTTP/2 protocol. nghttp2 is an open-source implementation of the HTTP/2 protocol, designed to improve performance and efficiency compared to HTTP/1.1. It offers several key features such as Multiplexing: Allows multiple requests to be sent concurrently over a single TCP connection, reducing latency and improving resource utilization, Header compression: Uses HPACK header compression to reduce the size of HTTP headers, further enhancing performance, and Server push: Enables servers to proactively push resources to clients, potentially reducing the number of round trips required to load a page.\\nWe use NGHTTP2 version 1.40.0 in our Node.js application, we typically wouldn't directly interact with it in JavaScript code. Instead, we would use a Node.js wrapper or binding that provides a JavaScript API for NGHTTP2.\\nOne such popular library is http2, which is included as part of Node.js core starting from version 8.8.0. Using the http2 module in Node.js allows us to leverage the features of the HTTP/2 protocol, such as header compression, multiplexing, and server push, in your applications. These features can help improve performance and efficiency, especially for applications that require low-latency communication or handle a large number of concurrent requests.\",\n",
       " \"UI Wiki Document:\\n\\nOpenssl == 1.9.0\\n\\nThe openssl dependency is a fork of OpenSSL to enable QUIC. OpenSSL is toolkit for general-purpose cryptography and secure communication. Node.js currently uses the quictls/openssl fork, which closely tracks the main openssl/openssl releases with the addition of APIs to support the QUIC protocol. See maintaining-openssl for more information.\\nFor our faceapp application, OpenSSL version 1.9.0 is used under the hood by the Node.js runtime to provide secure communication capabilities. However, we typically don't interact with OpenSSL directly in our Node.js application code. Instead, Node.js provides built-in modules such as crypto and tls, which utilize OpenSSL internally to provide cryptographic and secure communication features. For example, the crypto module provides cryptographic functionality, such as encryption, decryption, hashing, and digital signature generation and verification. Where as the tls module provides an implementation of the TLS (Transport Layer Security) and SSL (Secure Sockets Layer) protocols for secure communication. We use it to create secure servers and clients for handling HTTPS requests, secure WebSocket connections, and more.\\n\\n\\nEvent Simulator Wiki Document:\\n\\nMany users may be familiar with how software is packaged and installed on Linux and other systems using package managers. For example, to install a given Linux development library such as OpenSSL, a package manager is typically used (e.g., apt install openssl libssl-dev), which leads to shared libraries being installed under /usr/ lib/, development headers being installed under /usr/include/, etc. Programs that wish to use these libraries must link the system libraries and include the development headers. TES-3 is capabile of being installed in exactly the same way, and some downstream package maintainers have packaged TES-3 for some systems such as Ubuntu. However, as of this writing, the TES-3 project has not prioritized or standardized such package distribution, favoring instead to recommend a source download without a system-level install. This is mainly because most TES-3 users prefer to slightly or extensively edit or extend the TES-3 libraries, or to build them in specific ways (for debugging, or optimized for large-scale simulation campaign). Our build system provides an install command that can be used to install libraries and headers to system locations (usually requiring administrator privileges), but usually the libraries are just built and used from within the TES-3 build directory.\",\n",
       " \"Backend API Wiki Document:\\n\\nDependencies:\\n\\nspring-boot-starter-data-mongodb == 3.3.3\\n\\nSpring Boot Starter Data MongoDB is a Spring Boot module that provides an easy way to integrate MongoDB, a popular NoSQL document database, into Spring Boot application. It simplifies the process of connecting to a MongoDB database, performing CRUD operations, and using advanced features like data mapping, query derivation, and more.\\nTo use spring-boot-starter-data-mongodb version 3.3.3 in our resumelink application, we perform the following steps: We first create SpringBoot application, then we define domain model class. After that we create a repository framework. Then we configure MongoDB in application.properties. We then create a service class and controller class. In our application the spring-boot-starter-data-mongodb dependency is added to the project's build configuration (e.g., pom.xml for Maven or build.gradle for Gradle). This dependency provides the necessary libraries and configurations to integrate MongoDB into the Spring Boot application.\",\n",
       " 'Backend API Wiki Document:\\n\\nSpring framework == 6.0.15\\n\\nThe Spring Framework is a comprehensive Java framework facilitating the development of enterprise applications by promoting modularity, flexibility, and ease of integration. Its core container provides features such as dependency injection and inversion of control, simplifying component management. Spring MVC offers a robust model-view-controller architecture for building web applications, while Spring Data streamlines data access and manipulationacross various data stores. Spring Boot further accelerates development with auto-configuration and opinionated defaults, enabling rapid application setup and deployment.\\nIn our resumelink application the spring-boot-starter-web version 6.0.15 dependency is used in Spring Boot applications inherited through spring framework to quickly set up web applications. It includes dependencies required for building web applications using Spring MVC, including embedded servlet containers such as Tomcat, Spring Web, and other utilities. A basic example of how to use spring-boot-starter-web in a Spring Boot application is to add dependency to our project build configuration (e.g. â€˜pom.xmlâ€™ for Maven or build.gradle for Gradle)',\n",
       " \"Backend API Wiki Document:\\n\\nJsonwebtoken == 8.5.0\\n\\nio.jsonwebtoken is a Java library used for handling JSON Web Tokens (JWT). JWT is a compact, URL-safe means\\nof representing claims to be transferred between two parties. These claims are typically used to transmit information about an authenticated user or authorization data.\\nTo use JSON Web Tokens (JWTs) in our resumelink application, we utilize the io.jsonwebtoken version 8.5.0\\nlibrary. First, we add the necessary dependencies to our project's build configuration file (pom.xml for Maven or build.gradle for Gradle). Then, we create and validate JWTs using the library's APIs. This typically involves setting claims such as user ID, username, and expiration time when generating JWTs, and validating them upon receiving requests. Additionally, we integrate JWT authentication with Spring Security by implementing a custom filter to validate JWT tokens sent in requests.\",\n",
       " 'Backend API Wiki Document:\\n\\n5. ModelMapper (2.3.8)\\n\\nDescription: A powerful, convention-based object mapping library that simplifies the conversion of complex data types between different layers of the application.\\n\\nObject Mapping:\\nUtilizes ModelMapper for efficient and customizable object mapping between different layers of the application.\\n\\nmodelmapper == 2.4.2\\n\\nThe ModelMapper library is often used in Java applications to simplify the process of mapping data between different object models, such as mapping data from a database entity to a Data Transfer Object (DTO) or vice versa. For our resumelink application we use sub dependencies of modelmapper version 2.4.2 such as com.h2database which is a lightweight, in-memory SQL database for development and testing purposes. It provides features such as JDBC API compatibility, support for SQL syntax, and an embedded web-based console for database management. \\nFurthermore, in our context of our resumelink application, ModelMapper and com.h2database are often used together to simplify data management tasks. For example, ModelMapper is used to map entities retrieved from the H2 Database to DTOs used in RESTful endpoints. This combination allows for efficient data manipulation and transformation within the application, enabling us to focus on implementing business logic rather than dealing with low-level data handling intricacies.',\n",
       " 'Backend API Wiki Document:\\n\\n5. ModelMapper (2.3.8)\\n\\nDescription: A powerful, convention-based object mapping library that simplifies the conversion of complex data types between different layers of the application.\\n\\nObject Mapping:\\nUtilizes ModelMapper for efficient and customizable object mapping between different layers of the application.\\n\\nmodelmapper == 2.4.2\\n\\nThe ModelMapper library is often used in Java applications to simplify the process of mapping data between different object models, such as mapping data from a database entity to a Data Transfer Object (DTO) or vice versa. For our resumelink application we use sub dependencies of modelmapper version 2.4.2 such as com.h2database which is a lightweight, in-memory SQL database for development and testing purposes. It provides features such as JDBC API compatibility, support for SQL syntax, and an embedded web-based console for database management. \\nFurthermore, in our context of our resumelink application, ModelMapper and com.h2database are often used together to simplify data management tasks. For example, ModelMapper is used to map entities retrieved from the H2 Database to DTOs used in RESTful endpoints. This combination allows for efficient data manipulation and transformation within the application, enabling us to focus on implementing business logic rather than dealing with low-level data handling intricacies.',\n",
       " 'Event Simulator Wiki Document:\\n\\n3.2 Database support\\n\\nSQLite is recommended if you are using the statistics framework or if you are running LTE or NRsimulations (whichmake use of SQLite databases): We use sqlite version 3 in our toy event simulator application.',\n",
       " 'Event Simulator Wiki Document:\\n\\n3.2 Database support\\n\\nSQLite is recommended if you are using the statistics framework or if you are running LTE or NRsimulations (whichmake use of SQLite databases): We use sqlite version 3 in our toy event simulator application.',\n",
       " 'Event Simulator Wiki Document:\\n\\n1 Requirements\\n\\n1.1 Minimal requirements for release 3.36 and later\\nA C++ compiler (g++ version greater than 8 or clang++), Python 3, the CMake version greater than 3.10 build system, and a separate C++ building tool such as make, ninja-build, or Xcode are the minimal requirements for compiling the software. The tar and bunzip2 utilities are needed to unpack source file archives. If you want to instead use Git to fetch code, rather than downloading a source archive, then git is required instead.',\n",
       " 'Event Simulator Wiki Document:\\n\\nTES-3 library is written using C++ but provides support for Python environment. TES-3 is not compatible with Java compilers. TES-3 has minimal prerequisites for its most basic installation; namely, a C++ compiler, Python3 support, the CMake build system, and at least one of make, ninja, or Xcode build systems. However, some users will want to install optional packages to make use of the many optional extensions.\\n\\n1 Requirements\\n\\n1.1 Minimal requirements for release 3.36 and later\\nA C++ compiler (g++ version greater than 8 or clang++), Python 3, the CMake version greater than 3.10 build system, and a separate C++ building tool such as make, ninja-build, or Xcode are the minimal requirements for compiling the software. The tar and bunzip2 utilities are needed to unpack source file archives. If you want to instead use Git to fetch code, rather than downloading a source archive, then git is required instead.\\n\\n\\nBrowser Wiki Document:\\n\\nFor macOS development environment, you will need the prerequisites below to build Toy Browser on macOS 10.15+.\\n\\n1. macOS SDK 14.0 is needed. This is bundled with Xcode 15.0.',\n",
       " 'Event Simulator Wiki Document:\\n\\nTES-3 library is written using C++ but provides support for Python environment. TES-3 is not compatible with Java compilers. TES-3 has minimal prerequisites for its most basic installation; namely, a C++ compiler, Python3 support, the CMake build system, and at least one of make, ninja, or Xcode build systems. However, some users will want to install optional packages to make use of the many optional extensions.\\n\\n1 Requirements\\n\\n1.1 Minimal requirements for release 3.36 and later\\nA C++ compiler (g++ version greater than 8 or clang++), Python 3, the CMake version greater than 3.10 build system, and a separate C++ building tool such as make, ninja-build, or Xcode are the minimal requirements for compiling the software. The tar and bunzip2 utilities are needed to unpack source file archives. If you want to instead use Git to fetch code, rather than downloading a source archive, then git is required instead.\\n\\n\\nBrowser Wiki Document:\\n\\nFor macOS development environment, you will need the prerequisites below to build Toy Browser on macOS 10.15+.\\n\\n1. macOS SDK 14.0 is needed. This is bundled with Xcode 15.0.',\n",
       " 'Event Simulator Wiki Document:\\n\\nTES-3 library is written using C++ but provides support for Python environment. TES-3 is not compatible with Java compilers. TES-3 has minimal prerequisites for its most basic installation; namely, a C++ compiler, Python3 support, the CMake build system, and at least one of make, ninja, or Xcode build systems. However, some users will want to install optional packages to make use of the many optional extensions.\\n\\n1 Requirements\\n\\n1.1 Minimal requirements for release 3.36 and later\\nA C++ compiler (g++ version greater than 8 or clang++), Python 3, the CMake version greater than 3.10 build system, and a separate C++ building tool such as make, ninja-build, or Xcode are the minimal requirements for compiling the software. The tar and bunzip2 utilities are needed to unpack source file archives. If you want to instead use Git to fetch code, rather than downloading a source archive, then git is required instead.\\n\\n\\nBrowser Wiki Document:\\n\\nFor macOS development environment, you will need the prerequisites below to build Toy Browser on macOS 10.15+.\\n\\n1. macOS SDK 14.0 is needed. This is bundled with Xcode 15.0.',\n",
       " 'Event Simulator Wiki Document:\\n\\n3.11 XML-based version of the config store\\n\\nLibxml2 is needed for the XML-based Config Store feature. We utilize libxml2 version greater than 2.7 in our toy event simulator application.',\n",
       " 'Event Simulator Wiki Document:\\n\\n3.11 XML-based version of the config store\\n\\nLibxml2 is needed for the XML-based Config Store feature. We utilize libxml2 version greater than 2.7 in our toy event simulator application.',\n",
       " 'Event Simulator Wiki Document:\\n\\nTES-3 library is written using C++ but provides support for Python environment. TES-3 is not compatible with Java compilers. TES-3 has minimal prerequisites for its most basic installation; namely, a C++ compiler, Python3 support, the CMake build system, and at least one of make, ninja, or Xcode build systems. However, some users will want to install optional packages to make use of the many optional extensions.\\n\\nTES-3 is a set of C++ libraries (usually compiled as shared libraries) that can be used by C++ or Python programs to construct simulation scenarios and execute simulations. Users can also write programs that link other C++ shared libraries (or import other Python modules). Users can choose to use a subset of the available libraries; only the core library is strictly required. TES-3 uses the CMake build system (until release 3.36, the Waf build system was used). It can be built from command line or via a code editor program.\\n\\n1 Requirements\\n\\n1.1 Minimal requirements for release 3.36 and later\\n\\nA C++ compiler (g++ version greater than 8 or clang++), Python 3, the CMake version greater than 3.10 build system, and a separate C++ building tool such as make, ninja-build, or Xcode are the minimal requirements for compiling the software. The tar and bunzip2 utilities are needed to unpack source file archives. If you want to instead use Git to fetch code, rather than downloading a source archive, then git is required instead.\\n\\n2.2 Code linting\\n\\nSince TES-3.37 release, Clang-Format and Clang-Tidy are used to enforce the coding-style adopted by TES-3. Users can invoke these tools directly from the command-line or through the (utils/check-style-clang-format.py) check program. Moreover, clang-tidy is integrated with CMake, enabling code scanning during the build phase.',\n",
       " 'Event Simulator Wiki Document:\\n\\nTES-3 library is written using C++ but provides support for Python environment. TES-3 is not compatible with Java compilers. TES-3 has minimal prerequisites for its most basic installation; namely, a C++ compiler, Python3 support, the CMake build system, and at least one of make, ninja, or Xcode build systems. However, some users will want to install optional packages to make use of the many optional extensions.\\n\\nTES-3 is a set of C++ libraries (usually compiled as shared libraries) that can be used by C++ or Python programs to construct simulation scenarios and execute simulations. Users can also write programs that link other C++ shared libraries (or import other Python modules). Users can choose to use a subset of the available libraries; only the core library is strictly required. TES-3 uses the CMake build system (until release 3.36, the Waf build system was used). It can be built from command line or via a code editor program.\\n\\n1 Requirements\\n\\n1.1 Minimal requirements for release 3.36 and later\\n\\nA C++ compiler (g++ version greater than 8 or clang++), Python 3, the CMake version greater than 3.10 build system, and a separate C++ building tool such as make, ninja-build, or Xcode are the minimal requirements for compiling the software. The tar and bunzip2 utilities are needed to unpack source file archives. If you want to instead use Git to fetch code, rather than downloading a source archive, then git is required instead.\\n\\n2.2 Code linting\\n\\nSince TES-3.37 release, Clang-Format and Clang-Tidy are used to enforce the coding-style adopted by TES-3. Users can invoke these tools directly from the command-line or through the (utils/check-style-clang-format.py) check program. Moreover, clang-tidy is integrated with CMake, enabling code scanning during the build phase.',\n",
       " 'Browser Wiki Document:\\n\\n7. **jszip** (Version 3.6.1):\\nJSZip is a JavaScript library that provides functionality for creating, reading, and editing ZIP files directly in the browser. It allows developers to work with ZIP files entirely within a client-side JavaScript environment, without needing server-side processing.\\nIn our toy browser application, we utilize jszip version 3.6.1. To use jszip in our application first we need to include JsZip library in our project. This is done by downloading the librart and including it manually in our HTML file using a package manager, for example <script src=\"https://cdnjs.cloudflare.com/ajax/libs/jszip/3.6.0/jszip.min.js\"></script> then we create a new zip file using JSZip by nstantiating a new JSZip object.\\n\\nIn our toybrowser application we utilize webtorrent version 0.105.6. Similar to JSZip we nee dto include WebTorrent library in our project to use it. We include it via script tag, for example, <script src=\"https://cdn.jsdelivr.net/npm/webtorrent/webtorrent.min.js\"></script>. After this we create WebTorrent Client, for example, var client = new WebTorrent();',\n",
       " 'Browser Wiki Document:\\n\\n7. **jszip** (Version 3.6.1):\\nJSZip is a JavaScript library that provides functionality for creating, reading, and editing ZIP files directly in the browser. It allows developers to work with ZIP files entirely within a client-side JavaScript environment, without needing server-side processing.\\nIn our toy browser application, we utilize jszip version 3.6.1. To use jszip in our application first we need to include JsZip library in our project. This is done by downloading the librart and including it manually in our HTML file using a package manager, for example <script src=\"https://cdnjs.cloudflare.com/ajax/libs/jszip/3.6.0/jszip.min.js\"></script> then we create a new zip file using JSZip by nstantiating a new JSZip object.\\n\\nIn our toybrowser application we utilize webtorrent version 0.105.6. Similar to JSZip we nee dto include WebTorrent library in our project to use it. We include it via script tag, for example, <script src=\"https://cdn.jsdelivr.net/npm/webtorrent/webtorrent.min.js\"></script>. After this we create WebTorrent Client, for example, var client = new WebTorrent();',\n",
       " 'Browser Wiki Document:\\n\\n20. **webtorrent** (Version 0.105.6):\\n\\n- Enables streaming of torrents in the browser using WebRTC. WebTorrent is a streaming torrent client for the web browser and the Node.js environment. It allows users to stream torrents directly in the browser without the need for any additional plugins or extensions. \\nIn our toybrowser application we utilize webtorrent version 0.105.6. Similar to JSZip we nee dto include WebTorrent library in our project to use it. We include it via script tag, for example, <script src=\"https://cdn.jsdelivr.net/npm/webtorrent/webtorrent.min.js\"></script>. After this we create WebTorrent Client, for example, var client = new WebTorrent();',\n",
       " 'Browser Wiki Document:\\n\\n3. **json5** (Version 1.0.1):\\n\\n- JSON5 is an extension of the JSON format that allows for more human-readable JSON data. It adds support for features such as comments, trailing commas, and single-quoted strings, which are not allowed in standard JSON. JSON5 provides a more flexible and expressive way to write JSON-like data structures while maintaining compatibility with existing JSON parsers. We particularly utilize JSON5 version 1.0.1 for our browser application. To use JSON5 library in our browser application we first include them manually in our html file, for instance <script src=\"https://cdnjs.cloudflare.com/ajax/libs/json5/2.2.0/json5.min.js\"></script>. Then to parse JSON5 we use JSON5.parse() method.',\n",
       " 'Browser Wiki Document:\\n\\n2. **loader-utils** (Version 2.0.4):\\n\\n- Loader-utils is a utility library commonly used in Webpack loaders. It provides various helper functions for working with loader-related tasks, such as generating unique identifiers, parsing query parameters, and creating file paths. Loader-utils simplifies the development of Webpack loaders by offering a set of convenient functions for common tasks.\\n\\n3. **sass-loader** (Version 10.4.1):\\n\\n- Sass-loader is a webpack loader for compiling Sass/SCSS files into CSS. It allows developers to import Sass files directly into their JavaScript modules.\\n\\n8. **style-loader** (Version 2.0.0):\\n\\n- Style-loader is a webpack loader for injecting CSS styles into the DOM. It enables developers to import CSS files directly into their JavaScript modules.\\n\\nIn our toy browser application we use terser version 5.14.0. To utilize this library first we install it\\nin our browser application using npm bash script, for instance, npm install terser --save-dev. In our\\ncontext, it is used integrated with build tools such as webpack, roolup, to automate the minification process as part of our applicationâ€™s pipeline.\\n\\n14. **ts-loader** (Version 8.4.0):\\n\\n- Ts-loader is a webpack loader for compiling TypeScript files into JavaScript. It enables developers to import TypeScript files directly into their JavaScript modules.\\n\\n19. **url-loader** (Version 4.1.1):\\n\\n- Url-loader is a webpack loader for handling file imports in JavaScript modules. It allows developers to import files such as images and fonts as data URLs or file URLs depending on their size. \\n\\n20. **webpack** (Version 5.89.0):\\n\\n- Webpack is a module bundler for JavaScript applicationsIt takes modules with dependencies and generates static assets representing those modules, which are optimized for deployment in a browser environment.\\n\\nFor our resume application we particularly use webpack version 5.89.0. Similar to other dependencies we first install this dependency using bash script such as npm install webpack webpack-cli --save-dev. After that we configure the it using configuration file name webpack.config.js. This file specifies entry points, output paths, loaders, plugins, and other settings for webpack.\\n\\n21. **webpack-cli** (Version 5.1.4):\\n\\n- Webpack-cli is the command-line interface for Webpack. It provides utilities for running webpack builds, configuring webpack settings, and managing webpack projects from the command line.',\n",
       " 'Browser Wiki Document:\\n\\n11. **terser** (Version 5.14.0):\\n\\n- Terser is a JavaScript minifier and mangler. It is used to minimize the size of JavaScript bundles by removing unnecessary whitespace, comments, and renaming variables.\\n\\nIn our toy browser application we use terser version 5.14.0. To utilize this library first we install it in our browser application using npm bash script, for instance, npm install terser --save-dev. In our context, it is used integrated with build tools such as webpack, roolup, to automate the minificationprocess as part of our applicationâ€™s pipeline.',\n",
       " 'Browser Wiki Document:\\n\\n1. **reduxjs/toolkit** (Version 1.8.6):\\n\\n- Provides utilities for efficient Redux development, including simplified syntax and optimized performance.\\n\\n15. **redux-act** (Version 1.8.0):\\n\\n- Library for creating Redux actions and action creators.\\n\\n16. **redux-logger** (Version 3.0.6):\\n\\n- Middleware for logging Redux actions and state changes.\\n\\n17. **redux-thunk** (Version 2.3.0):\\n\\n- Middleware for handling asynchronous actions in Redux.\\n\\n19. **webext-redux** (Version 2.1.4):\\n\\n- Facilitates communication between Redux stores and WebExtensions.\\n\\n2. **redux** (Version 4.1.0):\\n\\n- Redux is a predictable state container for JavaScript applications. It provides a centralized store for managing application state and enables predictable state updates through actions and reducers.\\n\\nIn our toy browser application, we utilize redux version 4.1.0. We first install redus via npm using bash script, for instance, npm install redux react-redux. After this we set up redux store using createsStore() method. This store holds the state of our application. Then we define reducers. It specifies how our applicationâ€™s state changes in response to actions. We then connect redux to react using connect() method. Finally, we render our redux-connected components.\\n\\n15. **typesafe-actions** (Version 2.2.0):\\n\\n- Typesafe-actions is a library for creating type-safe Redux actions and action creators. It provides utilities for defining action types and generating action creator functions with TypeScript type safety.',\n",
       " 'Browser Wiki Document:\\n\\n1. **reduxjs/toolkit** (Version 1.8.6):\\n\\n- Provides utilities for efficient Redux development, including simplified syntax and optimized performance.\\n\\n15. **redux-act** (Version 1.8.0):\\n\\n- Library for creating Redux actions and action creators.\\n\\n16. **redux-logger** (Version 3.0.6):\\n\\n- Middleware for logging Redux actions and state changes.\\n\\n17. **redux-thunk** (Version 2.3.0):\\n\\n- Middleware for handling asynchronous actions in Redux.\\n\\n19. **webext-redux** (Version 2.1.4):\\n\\n- Facilitates communication between Redux stores and WebExtensions.\\n\\n2. **redux** (Version 4.1.0):\\n\\n- Redux is a predictable state container for JavaScript applications. It provides a centralized store for managing application state and enables predictable state updates through actions and reducers.\\n\\nIn our toy browser application, we utilize redux version 4.1.0. We first install redus via npm using bash script, for instance, npm install redux react-redux. After this we set up redux store using createsStore() method. This store holds the state of our application. Then we define reducers. It specifies how our applicationâ€™s state changes in response to actions. We then connect redux to react using connect() method. Finally, we render our redux-connected components.\\n\\n15. **typesafe-actions** (Version 2.2.0):\\n\\n- Typesafe-actions is a library for creating type-safe Redux actions and action creators. It provides utilities for defining action types and generating action creator functions with TypeScript type safety.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = list(eval_df['Ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The vulnerability in LangChain version 0.0.194, allowing remote code execution via specific functions such as from_math_prompt and from_colored_object_prompt, directly affects the security of LangChain's chains used in the ToyChat application. Exploitation of these chains could lead to unauthorized code execution, posing severe security risks such as data breaches or system compromise. Prompt action, including patching and implementing additional security measures, is necessary to mitigate these risks effectively.\",\n",
       " \"This LangChain vulnerability is critical due to its potential impact on both user interactions and system integrity. With the ability for remote attackers to execute arbitrary code, there's a high risk of compromising user interactions within the ToyChat application. Additionally, unauthorized code execution could jeopardize the integrity of the system, leading to data breaches or service disruptions.\",\n",
       " 'Users of hosted Streamlit app(s) are vulnerable to a reflected XSS vulnerability. An attacker could craft a malicious URL with Javascript payloads to a Streamlit app. The attacker could then trick the user into visiting the malicious URL and, if successful, the server would render the malicious javascript payload as-is, leading to an XSS.',\n",
       " 'Through the exploitation of the vulnerability in Streamlit, an attacker can craft a malicious URL containing JavaScript payloads targeting hosted Streamlit applications. By tricking users into visiting this malicious URL, the attacker can initiate a reflected XSS attack.',\n",
       " 'The vulnerability in Docker Desktop before version 4.12.0 allows remote code execution (RCE) via query parameters in the message-box route. Attackers exploit this flaw by injecting malicious code into specially-crafted URLs, which, when processed by Docker Desktop, execute the injected code within its environment. This vulnerability enables attackers to execute arbitrary commands or scripts with the privileges of Docker Desktop, potentially compromising the host system or Docker resources.',\n",
       " 'The vulnerability in Docker Desktop before version 4.12.0 allows remote code execution (RCE) by exploiting weaknesses in the message-box route. Attackers inject malicious code or commands into query parameters of URLs, which Docker Desktop processes without proper validation. Consequently, Docker Desktop inadvertently executes the injected code, enabling attackers to remotely execute arbitrary commands within the Docker Desktop environment.',\n",
       " \"This vulnerability introduces the risk of executing malicious git.exe or bash.exe from untrusted repositories on Windows systems. Such unauthorized execution of potentially harmful code poses a threat to the integrity of the chatbot, application's interactions with Git repositories. It compromises the reliability of operations performed using GitPython, as malicious code execution may lead to unexpected behavior, data corruption, or unauthorized access to sensitive information. \",\n",
       " 'The GitPython vulnerability allows execution of malicious code from untrusted repositories on Windows systems. This can lead to unauthorized access, privilege escalation, data exfiltration, application disruption, and compromise of Git operations within the chatbot, application.',\n",
       " \"An attacker could exploit the vulnerability in Joblib by manipulating the pre_dispatch flag in the Parallel() class to execute arbitrary code within our chatbot, application's environment. This could involve crafting malicious input data or parameters that, when processed by the vulnerable code, trigger the execution of unauthorized commands or scripts. By exploiting this vulnerability, the attacker could potentially gain unauthorized access to sensitive information, compromise the integrity of our chatbot, application, or disrupt its normal operation.\",\n",
       " \"The presence of the vulnerability in Joblib poses significant risks to our chatbot, application's reliability and data integrity. These risks include potential arbitrary code execution, data corruption, service disruption, unauthorized access, and loss of trust.\",\n",
       " 'Null assertion vulnerability could be exploited by an attacker by providing specially crafted input to the parser. This input could be structured in a way that it triggers null assertions within the codebase. When the vulnerable code encounters these null assertions, it may lead to unexpected behavior, potentially causing the application to crash or become unresponsive. By repeatedly sending such malicious input, an attacker could effectively create a denial of service condition, preventing legitimate users from accessing the service or causing significant disruption to its functionality.',\n",
       " 'The null assertion vulnerability in markdown-it poses significant risks to your chatbot, application and its users. Exploitation could lead to service disruption (unavailable to legetimate users), loss of functionality, data integrity concerns, and reputation damage.',\n",
       " 'The vulnerability in TOML could compromise the integrity of our TOML configuration files. Exploitation might lead to the injection of malicious content, causing unexpected behavior or altering critical settings. This could disrupt chatbot functionality, compromise reliability, and pose security risks.',\n",
       " 'Yes, exploitation of the vulnerability could result in the corruption or loss of vital configuration settings, directly impacting the behavior of our chatbot. Attackers may inject malicious content into TOML data, leading to erratic behavior or the loss of functionality. This could disrupt user experience and diminish the effectiveness of the chatbot.',\n",
       " 'The vulnerability in TQDM presents risks to the confidentiality, integrity, and availability of our data and services. Exploitation could lead to unauthorized access to sensitive data, tampering with data integrity, and service disruption.',\n",
       " 'Adversaries could exploit vulnerabilities in tqdm to target our chatbot, application by executing arbitrary code, escalating privileges, exfiltrating sensitive data, or causing service disruption. These vulnerabilities present significant risks to the security and functionality of our chatbot, potentially compromising confidentiality, integrity, and availability aspects of our system.',\n",
       " \"The vulnerability in NLTK could impact the security and performance of text processing tasks, such as tokenization and stemming, within our chatbot, application by potentially exposing it to inefficient regular expression complexity. This could lead to increased processing times and resource consumption, making the application more susceptible to denial-of-service attacks or performance degradation under heavy loads. Additionally, the vulnerability might also open up avenues for exploitation by attackers to manipulate or disrupt the text processing pipeline, compromising the integrity and security of the chatbot's functionality.\",\n",
       " \"The inefficient regular expression complexity vulnerability (CVE-2021-3842) in NLTK poses several potential risks to the overall reliability and integrity of our chatbot's functionality, especially concerning the processing of user input and information extraction. Firstly, the vulnerability may lead to performance degradation, causing delays or timeouts in text processing tasks. This could result in a poor user experience and decreased responsiveness of the chatbot, application. Secondly, attackers could exploit this vulnerability to execute denial-of-service attacks, overwhelming the system with malicious inputs that exploit the inefficiency of regex patterns. As a consequence, the chatbot's availability and uptime could be compromised. Moreover, the inefficient regex complexity might lead to inaccuracies in text processing, affecting the chatbot's ability to understand user queries correctly and provide accurate responses. Consequently, the reliability and trustworthiness of the chatbot's functionality could be undermined, potentially leading to user dissatisfaction and loss of credibility.\",\n",
       " 'The vulnerability in Pandas allows for arbitrary code execution because the GenerateSDFPipeline function does not properly sanitize user input. This function allows users to create dataframes that specify Python code to be executed. However, the function does not properly validate or escape this code, which means that an attacker can inject malicious code into the dataframe that will be executed by the SDFCodeExecutor.',\n",
       " 'To exploit the vulnerability in Pandas and execute arbitrary code, an attacker needs to Have access to the Pandas library, Be able to create or modify a dataframe, and Pass the malicious dataframe to the GenerateSDFPipeline function.',\n",
       " \"The vulnerability within Scipy, as described in CVE-2013-4251, pertains to the insecure creation of temporary directories by the scipy.weave component. This vulnerability can impact the security of our chatbot, application's statistical analysis tasks, especially when handling sensitive user data collected through the rating system. If exploited, attackers could potentially gain unauthorized access to the temporary directories and compromise the confidentiality and integrity of the user feedback data. This could lead to data leakage, manipulation, or unauthorized access, posing significant risks to the security of our chatbot, application and the privacy of user information.\",\n",
       " 'It can potentially allow attackers to exploit the temporary directories, gaining unauthorized access to sensitive user data. If exploited, attackers could manipulate or exfiltrate the user feedback data, compromising its confidentiality. Moreover, unauthorized access to the temporary directories could lead to the injection of malicious code or unauthorized modifications, jeopardizing the integrity of the data.',\n",
       " \"The vulnerability in NumPy jeopardizes the security of the chatbot, application's data preprocessing functions, particularly when handling user inputs or feedback ratings stored within NumPy arrays.\",\n",
       " 'The vulnerability in NumPy could compromise the overall security of our chatbot, application by allowing remote attackers to execute arbitrary code via crafted serialized objects. This could lead to unauthorized access, data breaches, and potential exploitation of sensitive user data processed by the chatbot.',\n",
       " \"The security of our chatbot, application's image processing tasks is compromised due to the vulnerability in Pillow, which allows for arbitrary code execution via the environment parameter, posing a risk of unauthorized access and potential exploitation of sensitive data within the application.\",\n",
       " \"The vulnerability in Pillow could significantly compromise the reliability and functionality of our chatbot, application's user interface, particularly due to its reliance on image manipulation for visual elements. If exploited, the vulnerability could lead to arbitrary code execution via the environment parameter, potentially allowing attackers to compromise the integrity of images used in the interface. This could result in distorted or maliciously altered visual elements, impacting user experience and potentially exposing users to security risks.\",\n",
       " 'Yes, there is a potential for arbitrary code execution if a malicious actor injects specially crafted YAML content into the configuration files parsed by PyYAML in the chatbot, application. This vulnerability exists due to an incomplete fix for CVE-2020-1747, allowing attackers to execute arbitrary code on the system through the python/object/new constructor',\n",
       " \"The presence of specific security measures like input validation or sandboxing to protect against the PyYAML vulnerability depends on the chatbot, application's implementation. However, some general approaches include: Input validation: Validating the YAML structure and sanitizing user-supplied data before parsing. Sandboxing: Running PyYAML in a restricted environment to limit potential damage.\",\n",
       " 'Yes, if the chatbot, application uses a vulnerable version of PyArrow (0.14.0 to 14.0.0) and processes untrusted data like user-supplied files, it is vulnerable to arbitrary code execution due to the CVE-2023-47248 vulnerability. To mitigate this risk, it is crucial to upgrade PyArrow, apply a hotfix if necessary, validate data, and restrict user input.',\n",
       " \"To secure your chatbot's data processing against the PyArrow vulnerability you need to Upgrade PyArrow to version 14.0.1 or later. If upgrading is not possible, apply the temporary pyarrow-hotfix package.Validate data and restrict user input to minimize risks. Consider sandboxing PyArrow for extra protection.\",\n",
       " 'The c-ares vulnerability could significantly impact our application by Causing denial-of-service: Attackers could exploit it to disrupt DNS resolution, making your application unavailable or hindering specific functionalities.Degrading performance: Even without a complete shutdown, the vulnerability could lead to slower response times and sluggish behavior, negatively impacting user experience.',\n",
       " 'The libuv vulnerability could indeed enable attackers to bypass security checks in your application and potentially access internal APIs or resources. By exploiting the truncation behavior of the uv_getaddrinfo function, attackers can craft payloads that resolve to unintended IP addresses, leading to Server-Side Request Forgery (SSRF) attacks.',\n",
       " \"The llhttp vulnerability poses a risk of HTTP Request Smuggling (HRS) attacks against your application. Attack scenarios include request smuggling via splitting, cache poisoning, session fixation, and bypassing security controls. Exploiting the inconsistency in the llhttp parser's handling of HTTP requests could lead to various security threats, highlighting the importance of prompt mitigation to protect against potential exploitation.\",\n",
       " 'Exploiting the nghttp2 vulnerability, which involves sending repeated large SETTINGS frames, could be feasible for attackers given sufficient resources and access to clients communicating with the vulnerable server. Detection and mitigation measures, such as dropping connections with large frames, may affect the feasibility, but effectiveness depends on implementation. The impact is significant, potentially causing denial of service by spiking CPU usage.',\n",
       " 'Our application is vulnerable to the openssl (node-openssl) package because we use openssl (node-openssl) package version 2.0.0 or earlier and it uses the opts argument with the verb field when interacting with the package.',\n",
       " 'The potential impact of the SpEL (Spring Expression Language) Injection vulnerability within the Spring Data MongoDB dependency on our application could be significant. If exploited, this vulnerability could allow attackers to manipulate SpEL expressions used in @Query or @Aggregation-annotated query methods. This could lead to unauthorized data access, injection of malicious code, or manipulation of database queries, potentially resulting in data breaches, data loss, or unauthorized modification of sensitive information stored in the MongoDB database.',\n",
       " 'Yes, our application is potentially vulnerable to the Denial-of-Service (DoS) vulnerability in Spring MVC if it is using Spring Framework version 6.0.15. The vulnerability affects versions 6.0.15 and 6.1.2 of Spring Framework, exposing applications to DoS attacks when certain conditions are met, including the use of Spring MVC and the presence of specific versions of Spring Security on the classpath.',\n",
       " \"With versions <= 8.5.1 of the jsonwebtoken library, there's a risk of misconfiguration leading to incorrect token verification. This misconfiguration could allow tokens to be successfully validated with a different algorithm and key combination than the one used for signing. Consequently, forged tokens could be accepted as valid, potentially leading to unauthorized access, spoofing, or other security breaches within our application.\",\n",
       " 'The potential impact of the vulnerability affecting com.h2database, a sub-dependency used in conjunction with ModelMapper in our Spring Boot application, can be significant. In this specific vulnerability, identified before version 2.1.210 of H2 Console, remote attackers may execute arbitrary code via a crafted JDBC URL.',\n",
       " 'The potential impact of the vulnerability affecting the org.h2.util.JdbcUtils.getConnection method in the H2 database on our Spring Boot application could be severe. This vulnerability allows an attacker to pass a JNDI driver name and URL, which could lead to remote code execution. Exploiting this vulnerability, an attacker could execute arbitrary code on the system hosting our application, potentially resulting in unauthorized access, data theft or manipulation, and even a complete compromise of the application and its underlying infrastructure.',\n",
       " \"A heap use-after-free vulnerability was found in SQLite's jsonParseAddNodeArray() function. Exploiting this flaw could lead to a denial of service if a victim is tricked into providing specially crafted input. This issue affects TES-3, which relies on SQLite for its database operations.\",\n",
       " \"SQLite JDBC, utilized for SQLite database operations in Java, had a remote code execution vulnerability via JDBC URL impacting versions 3.6.14.1 through 3.41.2.1, resolved in 3.41.2.2. Although TES-3 depends on SQLite for its database operations, being written in C++, it doesn't utilize SQLite via JDBC and thus isn't susceptible to remote code execution through SQLite.\",\n",
       " 'GCC and G++ versions 3.3.3 and earlier lack full support for detecting all types of integer overflows when using the -ftrapv compiler option. This vulnerability, tracked as CVE-2000-1219, could leave applications susceptible to overflow-related vulnerabilities. TES-3, which requires at least GCC version 8, is not affected by this issue.',\n",
       " 'Parsing a file may lead to disclosure of user information for Xcode version < 14.0. This vulnerability is tracked as CVE-2022-32920. TES-3 does not have any minimum version requirement for Xcode. Hence, your system may be vulnerable to disclosure of user information under the current xcode version.',\n",
       " \"An injection vulnerability was fixed in Xcode 14.1 with improved input validation (CVE-2022-42797), preventing potential root privilege escalation. TES-3 doesn't specify a minimum Xcode version, leaving systems potentially vulnerable to code injection attacks due to older Xcode versions.\",\n",
       " \"Jenkins Xcode integration Plugin versions 2.0.14 and earlier lack proper configuration to prevent XML external entity (XXE) attacks. However, TES-3 doesn't utilize this Jenkins plugin, so the system is not vulnerable to this issue.\",\n",
       " \"A use-after-free vulnerability in libxml2 before 2.9.5, as seen in Google Chrome before 63.0.3239.84, allowed potential heap corruption via a crafted HTML page. While TES-3 utilizes libxml2 for its XML-based Config Store feature, it's not an web application accessed through browser, like Google Chrome. Thus, libxml2 cannot be exploited to corrupt heap memory in TES-3.\",\n",
       " \"Libxml2, as employed in Red Hat JBoss Core Services in recovery mode, could be exploited by context-dependent attackers to cause denial of service via stack consumption with a crafted XML document. However, TES-3, not being a web application and not utilizing Red Hat JBoss Core, isn't vulnerable to this type of denial of service attack stemming from libxml2 vulnerabilities.\",\n",
       " \"CMake installs its x86 Linux binaries via HTTP, potentially exposing it to MITM attacks, leading to remote code execution (RCE) if attackers replace requested binaries. However, TES-3, utilizing CMake for C++ compilation management, doesn't communicate with web servers over HTTP as it's a local application. Therefore, it's not vulnerable to such attacks.\",\n",
       " \"CMake before version 2.2.0-r1 on Gentoo Linux is susceptible to an untrusted search path vulnerability, enabling local users in the portage group to elevate privileges through a malicious shared object in the Portage temporary build directory included in the RUNPATH. As TES-3 runs on Linux and utilizes the CMake build system for C++ compilation, it's indeed vulnerable to this exploit.\",\n",
       " 'Versions of jszip prior to 3.8.0, utilizing the loadAsync function, are susceptible to Directory Traversal via a manipulated ZIP archive. Toy Browser relies on jszip version 3.7.1, which employs the loadAsync function. Consequently, your application may be vulnerable to directory traversal through a manipulated ZIP archive. ',\n",
       " 'Package jszip before 3.7.0 can be exploited by crafting a new zip file with filenames set to Object prototype values (e.g __proto__, toString, etc) results in a returned object with a modified prototype instance. As Toy Browser uses jszip version 3.6.1, our application can be exploited using jszip package.',\n",
       " 'Toy browser utilizes WebTorrent version 0.105.6. WebTorrent versions prior to 0.107.6 are vulnerable to XSS attacks in the HTTP server through the title or file name. Consequently, the Toy browser is susceptible to XSS risks on its HTTP server.',\n",
       " 'The parse method of the JSON5 library before and including versions 1.0.1 and 2.2.1 does not restrict parsing of keys named `__proto__`, allowing specially crafted strings to pollute the prototype of the resulting object. This causes Prototype Pollution. Toy Browser does uses version 1.0.1, however it currently does not uses __proto__ key. Hence, an attacker cannot currently target our application.',\n",
       " 'The Webpack library loader-utils 2.0.0 contains a prototype pollution vulnerability within the function parseQuery in parseQuery.js. However, Toy Browser does not utilize the parseQuery function. Therefore, attackers cannot execute a prototype pollution attack on Toy Browser by exploiting Webpack.',\n",
       " 'Toy browser uses terser package with version 5.14.0, The package terser before 4.8.1, from 5.0.0 and before 5.14.2 are vulnerable to Regular Expression Denial of Service (ReDoS) due to insecure usage of regular expressions. Hence, an adversary can exploit regular expression to launch ReDoS on toy browser application. ',\n",
       " 'The Redux framework has been identified as vulnerable to Auth. (admin+) Stored Cross-Site Scripting (XSS) attacks. However, Toy Browser utilizes the Redux framework without the iControlWP Article Directory Redux plugin. Consequently, attackers cannot exploit the Redux framework to perform XSS attacks on Toy Browser.',\n",
       " \"The Redux Framework plugin, before version 4.2.11 for WordPress, registered several AJAX actions accessible to unauthenticated users through the `includes` function in `redux-core/class-redux-core.php`. These actions were unique to each site but predictable, as they were generated from an md5 hash of the site URL with a known salt value of '-redux' and an md5 hash of the previous hash with a known salt value of '-support'. While Toy Browser utilizes the Redux framework, it does not use the redux-core package. Consequently, adversaries cannot predict MD5 hash values to improperly authenticate unauthenticated users.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_prompt = \"\"\"\n",
    "You are a helpful honest assistant and you answer questions based only on the information that is given to you.\n",
    "You do not add anything in your answer that is not instructed to you. \n",
    "If you cannot answer you simply say you do not know the answer. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_knowledge_retrieval_query_prompt = \"\"\"\\n\n",
    "Question: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_contextualized_completion_prompt = \"\"\"\n",
    "Given the following global_knowledge that is threat or vulnerability report retrieved from the internet relevant for the question and\n",
    "current infrastructure details of running applications such as used dependencies, used libraries, used version and use case of that dependency in the application as local_knowledge.\n",
    "Answer the question from the information provided in global_knowledge and local_knowledge.\n",
    "Do not add anything that is not given in the global_knowledge or local_knowledge. \n",
    "Only include answer without mentioning its information source. \n",
    "If there is no relation between the global_knowledge and the local_knowledge, or you cannot understand the context,\n",
    "then you say more information is required to answer the question, while mentioning what information is required. \n",
    "\n",
    "global_knowledge: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_knowledge_addition_prompt = \"\"\"\\n\n",
    "local_knowledge:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_contextualization_prompt(global_knowledge_str: str, local_knowledge: str, query: str) \\\n",
    "        -> str:\n",
    "    # global_knowledge_str = '\\n'.join([x for x in global_knowledge])\n",
    "    final_prompt = (generic_prompt + final_contextualized_completion_prompt + global_knowledge_str\n",
    "                    + local_knowledge_addition_prompt + local_knowledge + local_knowledge_retrieval_query_prompt\n",
    "                    + query + \"\\n\\nAnswer: \")\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_list = []\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    test_case_dict = {\n",
    "        'question': questions[i],\n",
    "        'global_knowledge': global_knowledge[i],\n",
    "        'local_knowledge': local_knowledge[i],\n",
    "        'ground_truth': ground_truth[i],\n",
    "        'input_prompt': get_final_contextualization_prompt(global_knowledge[i], local_knowledge[i], questions[i])\n",
    "    }\n",
    "    \n",
    "    final_eval_list.append(test_case_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful honest assistant and you answer questions based only on the information that is given to you.\n",
      "You do not add anything in your answer that is not instructed to you. \n",
      "If you cannot answer you simply say you do not know the answer. \n",
      "\n",
      "Given the following global_knowledge that is threat or vulnerability report retrieved from the internet relevant for the question and\n",
      "current infrastructure details of running applications such as used dependencies, used libraries, used version and use case of that dependency in the application as local_knowledge.\n",
      "Answer the question from the information provided in global_knowledge and local_knowledge.\n",
      "Do not add anything that is not given in the global_knowledge or local_knowledge. \n",
      "Only include answer without mentioning its information source. \n",
      "If there is no relation between the global_knowledge and the local_knowledge, or you cannot understand the context,\n",
      "then you say more information is required to answer the question, while mentioning what information is required. \n",
      "\n",
      "global_knowledge: \n",
      "Auth. (admin+) Stored Cross-Site Scripting (XSS) vulnerability in iControlWP Article Directory Redux plugin <= 1.0.2 versions.\n",
      "\n",
      "local_knowledge:\n",
      "Browser Wiki Document:\n",
      "\n",
      "1. **reduxjs/toolkit** (Version 1.8.6):\n",
      "\n",
      "- Provides utilities for efficient Redux development, including simplified syntax and optimized performance.\n",
      "\n",
      "15. **redux-act** (Version 1.8.0):\n",
      "\n",
      "- Library for creating Redux actions and action creators.\n",
      "\n",
      "16. **redux-logger** (Version 3.0.6):\n",
      "\n",
      "- Middleware for logging Redux actions and state changes.\n",
      "\n",
      "17. **redux-thunk** (Version 2.3.0):\n",
      "\n",
      "- Middleware for handling asynchronous actions in Redux.\n",
      "\n",
      "19. **webext-redux** (Version 2.1.4):\n",
      "\n",
      "- Facilitates communication between Redux stores and WebExtensions.\n",
      "\n",
      "2. **redux** (Version 4.1.0):\n",
      "\n",
      "- Redux is a predictable state container for JavaScript applications. It provides a centralized store for managing application state and enables predictable state updates through actions and reducers.\n",
      "\n",
      "In our toy browser application, we utilize redux version 4.1.0. We first install redus via npm using bash script, for instance, npm install redux react-redux. After this we set up redux store using createsStore() method. This store holds the state of our application. Then we define reducers. It specifies how our applicationâ€™s state changes in response to actions. We then connect redux to react using connect() method. Finally, we render our redux-connected components.\n",
      "\n",
      "15. **typesafe-actions** (Version 2.2.0):\n",
      "\n",
      "- Typesafe-actions is a library for creating type-safe Redux actions and action creators. It provides utilities for defining action types and generating action creator functions with TypeScript type safety.\n",
      "\n",
      "Question: Is it possible for an adversary to predict the MD5 hash value in order to wrongly authenticate an unauthenticated user?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(final_eval_list[-1]['input_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt_list = []\n",
    "for i in final_eval_list:\n",
    "    input_prompt_list.append(str(i['input_prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful honest assistant and you answer questions based only on the information that is given to you.\n",
      "You do not add anything in your answer that is not instructed to you. \n",
      "If you cannot answer you simply say you do not know the answer. \n",
      "\n",
      "Given the following global_knowledge that is threat or vulnerability report retrieved from the internet relevant for the question and\n",
      "current infrastructure details of running applications such as used dependencies, used libraries, used version and use case of that dependency in the application as local_knowledge.\n",
      "Answer the question from the information provided in global_knowledge and local_knowledge.\n",
      "Do not add anything that is not given in the global_knowledge or local_knowledge. \n",
      "Only include answer without mentioning its information source. \n",
      "If there is no relation between the global_knowledge and the local_knowledge, or you cannot understand the context,\n",
      "then you say more information is required to answer the question, while mentioning what information is required. \n",
      "\n",
      "global_knowledge: \n",
      "An issue in Harrison Chase langchain v.0.0.194 and before allows a remote attacker to execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions.\n",
      "\n",
      "local_knowledge:\n",
      "Chatbot Wiki Document:\n",
      "\n",
      "The application's database is structured as a vector database, specifically using Chroma DB, and document embeddings are generated using LangChain and OpenAI's \"text_ada_002.\"\n",
      "\n",
      "3. Database Structure\n",
      "- The database is organized in the form of a vector database using Chroma DB.\n",
      "- Document embeddings are generated using LangChain and OpenAI's \"text_ada_002\" to represent text documents as vectors.\n",
      "\n",
      "3. Document Embeddings\n",
      "- Document embeddings are created using LangChain and OpenAI's \"text_ada_002\" to represent text documents\n",
      "as vectors.\n",
      "\n",
      "Dependencies\n",
      "ChatBot relies on the following Python libraries and versions:\n",
      "\n",
      "Langchain == 0.0.194\n",
      "\n",
      "LangChain is a comprehensive framework designed for the development of applications powered by language models. It facilitates the creation of context-aware applications that can connect language models to various sources of context, such as prompt instructions, few-shot examples, or relevant content, enabling them to provide more accurate and relevant responses. Additionally, LangChain enables applications to reason by relying on language models to determine how to respond based on the provided context and what actions to take. The framework comprises several components, including LangChain Libraries, which consist of Python and JavaScript libraries containing interfaces, integrations for various components, a basic runtime for combining these components into chains and agents, and pre-built implementations of chains and agents. Furthermore, LangChain offers LangChain Templates, a collection of easily deployable reference architectures for a wide range of tasks, LangServe, a library for deploying LangChain chains as a REST API, and LangSmith, a developer platform that facilitates debugging, testing, evaluation, and monitoring of chains built on any LLM framework while seamlessly integrating with LangChain.\n",
      "\n",
      "For our ChatBot application we utilize LangChain framework. We use Langchain version 0.0.194 for our application. It provides the us platform to adapt language model with flexibility to our specific context. We utilize some of the core components within the langchain to build context-aware language models systems. To achieve this, we use core components including LLM interface, chains, prompt templates, retrieval modules, memory. In case of LLM interface, we interface with proprietary models such as GPT to make API calls. We leverage gpt-3.5-turbo as our base generator model. Chains are the fundamental principle that holds various AI components in LangChain to provide context-aware responses. A chain is a series of automated actions from the user's query to the model's output. Chain in our case implements Program-Aided Language Models (PAL) for generating code solutions. This is utilized to answer math or code related questions in our interface. PALChain reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.\n",
      "\n",
      "Question: Considering the use of LangChain's chains for implementing Program-Aided Language Models (PAL) within the chatbot, application, how susceptible are these chains to exploitation?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(input_prompt_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(input_prompt_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = get_gpt3_5_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['gpt_35_completion'] = completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_completion_file_path = \"./data/evaluation/LocalIntel_GPT_35_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_excel(gpt_35_completion_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_completions = get_mistral_7b_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(mistral_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_7b_completion_file_path = \"./data/evaluation/LocalIntel_mistral_7b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['mistral_7b_completion'] = mistral_completions\n",
    "    eval_df.to_excel(mistral_7b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_qwen_7b_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_completions = get_qwen_7b_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(qwen_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_7b_completion_file_path = \"./data/evaluation/LocalIntel_qwen_7b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['qwen_7b_completion'] = qwen_completions\n",
    "    eval_df.to_excel(qwen_7b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_westlake_7b_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "westlake_completions = get_westlake_7b_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(westlake_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "westlake_7b_completion_file_path = \"./data/evaluation/LocalIntel_westlake_7b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['westlake_7b_completion'] = westlake_completions\n",
    "    eval_df.to_excel(westlake_7b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "westlake_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_westseverus_7b_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "westseverus_completions = get_westseverus_7b_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(westseverus_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "westseverus_7b_completion_file_path = \"./data/evaluation/LocalIntel_westseverus_7b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['westseverus_7b_completion'] = westseverus_completions\n",
    "    eval_df.to_excel(westseverus_7b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "westseverus_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_completions = get_llama2_7b_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "size = len(llama_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_7b_completion_file_path = \"./data/evaluation/LocalIntel_llama_7b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['llama_7b_completions'] = llama_completions\n",
    "    eval_df.to_excel(llama_7b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBased on the information provided in the global knowledge and local knowledge, the chains implemented in the chatbot application using LangChain's framework are susceptible to exploitation through the from_math_prompt and from_colored_object_prompt functions in Harrison Chase langchain v.0.0.194 and before. These functions allow a remote attacker to execute arbitrary code, which could potentially lead to unauthorized access or manipulation of the chatbot's responses. To mitigate this vulnerability, it is recommended to upgrade to a later version of Harrison Chase langchain that addresses this issue. Additionally, implementing additional security measures such as input validation and sanitization can help prevent exploitation of these vulnerabilities.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ai-Maven-Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The given threat or vulnerability report highlights an issue in Harrison Chase langchain version v.0.0.194 and older, where a remote attacker can execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions. While the described vulnerability is related to LangChain, it is essential to note that the specific context of the chatbot application provided does not directly indicate its version or whether it falls within the vulnerable range. To determine the susceptibility of the chatbot application to this exploitation, more information about the actual version used in the application is required.\n",
      "\n",
      "The identified vulnerability in Harrison Chase langchain v.0.0.194 and before allows a remote attacker to execute arbitrary code through specific functions, posing a potential threat to user interactions and system integrity if exploited. The precise impact on user interactions and system stability in the given ChatBot application setup, using LangChain version 0.0.194, cannot be determined without a thorough assessment of the application's security measures and its implementation of LangChain components. More information is required to evaluate the criticality of this vulnerability in the context of the specific ChatBot application.\n",
      "\n",
      "Exploitation of the XSS vulnerability in Streamlit version 0.63.0 used by the chatbot application could lead to malicious JavaScript code execution on users' browsers when they visit a specially crafted malicious URL related to the Streamlit app. This may result in unauthorized actions being taken within the user's browser session or unintended exposure of sensitive information. It is crucial to update to the patched version 0.81.0 to avoid these security risks.\n",
      "\n",
      "An attacker taking advantage of the exploited XSS vulnerability in Streamlit, during the mentioned versions, could create a malicious URL with JavaScript payloads that are directed to a Streamlit app. Once a user visits the crafted malicious URL, the server would render the malicious JavaScript payload as-is within the app, leading to a successful XSS attack. This could potentially allow the attacker to execute arbitrary code within the victim's web browser, conduct session hijacking, or gain access to sensitive data.\n",
      "\n",
      "The vulnerability in Docker Desktop before 4.12.0 could potentially impact the security of the chatbot application as it allows Remote Code Execution (RCE) via query parameters in message-box route. Since the chatbot uses Docker version 3.1 in a Linux (Ubuntu 22.04) environment, if the system has not been updated to a newer version beyond 4.12.0, it might be at risk from this vulnerability. It is crucial to ensure the Docker Desktop version is up-to-date to mitigate such security risks.\n",
      "\n",
      "The given information focuses on an identified vulnerability in Docker Desktop versions before 4.12.0, allowing Remote Code Execution (RCE) via query parameters in the message-box route. To elaborate on RCE, it's a severe security flaw that enables an attacker to execute arbitrary code on the targeted system, usually resulting from improper input validation or insufficient access restrictions.\n",
      "\n",
      "In this specific context, an attacker could exploit this vulnerability by crafting malicious query parameters for the message-box route within Docker Desktop, gaining unauthorized code execution capabilities. As the details about the message-box route's functionality are not provided, it is difficult to describe the exact exploitation process. However, the vulnerability puts Docker Desktop users at risk until they upgrade to a patched version (4.12.0 or later). \n",
      "\n",
      "More information is required to understand the intricate nature of the exploit and the message-box route's role in the Docker Desktop application.\n",
      "\n",
      "The vulnerability in GitPython, specifically CVE-2023-40590, may pose a risk for the chatbot application's interactions with Git repositories on Windows systems. Due to an untrusted search path and usage of `bash.exe` to interpret hooks, a malicious `git.exe` or `bash.exe` from an untrusted repository could be executed. The given local_knowledge reveals that the chatbot application uses GitPython version 3.1.40, which does not include the complete fix for this vulnerability. Therefore, the impact on the chatbot's reliability and security when interacting with Git repositories could be significant if it operates on Windows and employs the susceptible features. Updating to version 3.1.41, which has the incomplete fix, or a future version with a complete fix is recommended for improved security. More information is required to determine the exact likelihood of exploitation and potential consequences in the chatbot's context.\n",
      "\n",
      "The given vulnerability, CVE-2023-40590, relates to GitPython running on Windows with insecure usage of `git` or `bash.exe`. In this scenario, malicious executables from untrusted repositories could be run due to an untrusted search path. For the chatbot application using GitPython 3.1.40, if it operates on Windows and employs such susceptible methods, there is a potential risk. However, as the chatbot app runs version 3.1.40, it has not been fully patched against this issue, as the fix is only 'incomplete' in the mentioned global_knowledge. Thus, extra caution or an update to the latest version 3.1.41 should be considered to mitigate this vulnerability impacting the chatbot app. More precisely, updating to the patched version would remove the aforementioned risk.\n",
      "\n",
      "An attacker could potentially exploit the joblib vulnerability in versions before 1.2.0 by manipulating inputs in a way that triggers the Arbitrary Code Execution vulnerability in the Parallel() class' pre_dispatch flag through the eval() statement. However, since your chatbot application uses joblib version 1.0.0, which is not within the vulnerable range, this specific attack vector is not applicable to your current setup. More information regarding the exact use case of this vulnerability exploitation within a similar setup is needed to further elaborate on the potential attack scenario.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The potential risk to the chatbot application due to the JobLib vulnerability in version 0 and before 1.2.0 is Arbitrary Code Execution. Since the chatbot runs with version 1.0.0 which is not in the vulnerable range, it is not directly exposed to this threat. However, it is important to keep monitoring and updating JobLib to ensure security, as newer vulnerabilities may be discovered in the future affecting later versions.\n",
      "More information is required to assess the specific impact of this vulnerability on chatbot's application's reliability and data integrity in older or outdated dependencies or libraries used in conjunction with JobLib 1.0.0.\n",
      "\n",
      "Before version 2.2.0 of markdown-it-py, an attacker could potentially exploit the null assertion vulnerability by providing specially crafted input that would trigger the issue, leading to a Denial of Service situation. More details about the exact attack method or required input are not given, requiring additional information to comprehensively answer this question.\n",
      "\n",
      "In the given context of using markdown-it-py version 2.1.0 in the Chatbot application, there is a potential threat of Denial of Service (DoS) attack due to a vulnerability before v2.2.0, specifically when dealing with null assertions from specially crafted input. While the exact impact on the chatbot and its users cannot be described without considering specific application details, a DoS attack could potentially disrupt the service provided by the chatbot, causing unavailability or degraded functionality, negatively affecting the chatbot's users. More information is required to specify the extent of such an impact on the application and users.\n",
      "\n",
      "The given global_knowledge describes a Denial of Service (DOS) attack vulnerability associated with using jackson-dataformats-text for parsing TOML data in specific scenarios. It does not directly relate to the impact on the integrity of the TOML configuration data itself. More information is required to determine the connection between the vulnerability and the integrity of the configuration data in this context.\n",
      "\n",
      "TOML exploitation through a vulnerable parser, as mentioned in the global_knowledge, could potentially lead to a Denial of Service (DOS) attack, disrupting the parser's functionality. However, the information given does not specifically connect this exploit to corruption or loss of important configuration settings in the context of your chatbot application. More details about the implementation and specific dependencies involved are needed to determine whether such an attack could result in the described adverse effects on your chatbot's behavior.\n",
      "\n",
      "The mentioned TQDM vulnerability in version 4.4.1 exposes a risk related to local users potentially executing arbitrary code through a crafted repo with a malicious git log in the current working directory. While this vulnerability doesn't directly impact the confidentiality, integrity, or availability of data and services, it might be exploited to compromise the security of the system running the application. More information is required to specifically assess the impact on confidentiality, integrity, and availability. Focusing on the vulnerability aspect, an update to a newer, non-vulnerable version of TQDM is recommended.\n",
      "\n",
      "Adversaries could potentially exploit the vulnerability in tqdm version 4.4.1 by crafting a malicious git repository with a manipulated git log placed in the current working directory of the chatbot application. This could allow them to execute arbitrary code within the context of the application, which may lead to compromising its security and functionality.\n",
      "```\n",
      "\n",
      "The given threat or vulnerability, \"Inefficient Regular Expression Complexity\" in NLTK, primarily relates to the performance aspect rather than the security of the library. However, its impact on text processing tasks like tokenization and stemming in your chatbot application may include possible decreased processing speed due to inefficient regular expression usage in some functions. This could result in slower response times for these specific tasks within the chatbot, but it doesn't compromise the security of your application. More information is required to determine whether there are any potential workarounds or updates that could mitigate this performance impact.\n",
      "\n",
      "The identified threat or vulnerability related to NLTK is its susceptibility to Inefficient Regular Expression Complexity. While this information doesn't directly address the potential impact on chatbot functionality, it indicates a potential performance or efficiency concern in the underlying regular expression capabilities of NLTK. However, more information is required to evaluate its specific influence on your chatbot's overall reliability and integrity in processing user input and extracting information, as well as any mitigation strategies.\n",
      "\n",
      "The vulnerability in PandasAI's GenerateSDFPipeline in synthetic_dataframe allows attackers to induce the generation of arbitrary Python code by creating a dataframe with an English language specification of the desired code. This code is then executed by the SDFCodeExecutor, potentially leading to exploitation. It is relevant to the mentioned version 1.5.17, while the provided local_knowledge utilizes an older version of Pandas (1.4.3) without directly involving PandasAI.\n",
      "\n",
      "For an attacker to exploit the PandasAI (pandas-ai) vulnerability allowing arbitrary code execution, they need to manipulate a dataframe in the GenerateSDFPipeline feature within synthetic_dataframe, which is present up to version 1.5.17. The malicious dataframe must provide an English language specification of the Python code the attacker intends to execute. The vendor previously made an attempt to restrict code execution in response to a different issue, CVE-2023-39660, but that does not directly affect the exploitation of the described vulnerability. The given context does not mention the specific chatbot application version that is vulnerable, so it's crucial to ensure updating to a non-vulnerable version of the library if using the affected range.\n",
      "\n",
      "The vulnerability in the pre-0.12.1 version of scipy.weave component may create insecure temporary directories, but as the provided local_knowledge states the Chatbot application uses Scipy version 0.9.0, which doesn't include the vulnerable scipy.weave component. Since the vulnerability doesn't exist in the used version of Scipy, it doesn't impact the security of your chatbot, or the statistical analysis tasks when handling sensitive user data collected through the rating system. However, it's crucial to maintain the latest stable version of Scipy to benefit from future security updates and improvements.\n",
      "\n",
      "The vulnerable version of Scipy, specifically its weave component before 0.12.1, poses a risk in the confidentiality and integrity of user feedback data processed in the chatbot application due to insecure creation of temporary directories. This vulnerability could potentially allow unauthorized access or manipulation of sensitive data during the statistical analysis stage, carried out using the Scipy library. More secure versions of Scipy should be implemented to mitigate these risks. Additional controls and encryption methods may also be necessary to further protect user data.\n",
      "\n",
      "The vulnerability in NumPy, specifically before version 1.16.0, poses a risk to the security of the chatbot application as it allows remote attackers to potentially execute arbitrary code through a crafted serialized object, with examples given using numpy.load call. When handling user inputs or feedback ratings stored within NumPy arrays, there is a possibility for malicious input to exploit this issue if not properly addressed and updated beyond the vulnerable version. More attention and security measures should be taken to protect against such threats in the context of data preprocessing functions using NumPy arrays in the chatbot application. \n",
      "\n",
      "More information is required to assess the exact implementation and mitigation measures taken in the chatbot's data preprocessing functions to deal with this vulnerability.\n",
      "\n",
      "The vulnerability in NumPy 1.16.0 and earlier, allowing remote attackers to execute arbitrary code via crafted serialized objects, poses a potential risk to the overall security posture of an application using these affected versions, including a chatbot. However, considering the context provided and the specified use of Numpy version 1.15.0, the chatbot application might not be directly exposed to this issue, as it runs an older, presumably patched, version. Still, extra vigilance in handling serialized data or dependencies and prompt updates to the latest secure version are advisable to mitigate potential security threats. More information is required regarding the specifics of the chatbot's dependencies and usage of NumPy to provide a more definitive answer.\n",
      "\n",
      "The security of your chatbot, application's image processing tasks may be affected due to the vulnerability in Pillow up to version 10.1.0 (specifically allowing Arbitrary Code Execution via the environment parameter). Since your application is using Pillow version 10.0.0, it is necessary to verify if this version is not affected by CVE-2022-22817 (expression parameter vulnerability) but is still vulnerable to the mentioned Arbitrary Code Execution exploit through the environment parameter. Updating to a patched version would mitigate these risks. More information is required to understand the exact implementation details and assess the overall impact on your application.\n",
      "\n",
      "The vulnerability in Pillow up to version 10.1.0 could potentially impact the chatbot application if exploited, as it allows Arbitrary Code Execution through the environment parameter within PIL.ImageMath.eval. Since the chatbot app heavily depends on Pillow for image manipulation tasks related to its user interface, it might expose the application to security risks associated with this vulnerability. These risks could manifest through malicious manipulation of the mascot, logo, or banner images used in the interface, thereby affecting the overall reliability and functionality of the chatbot. It is essential to ensure that the chatbot's infrastructure is updated to use a Pillow version beyond 10.1.0, which addresses this issue, to mitigate the associated risks. More information is required about the specific implementation of Pillow and its integration with the chatbot to provide a comprehensive evaluation of the potential impact.\n",
      "\n",
      "Yes, there is a potential for arbitrary code execution due to the vulnerability in PyYAML library before version 5.4. As the chatbot application uses PyYAML version 5.1, it may be vulnerable to this flaw if it processes untrusted input from YAML files. The flaw allows an attacker to execute arbitrary code by abusing the python/object/new constructor. The recommended action is to update PyYAML to a version after 5.3, which contains the necessary fixes to mitigate this risk.\n",
      "\n",
      "More information is required to answer the question specifically about security measures in place for the chatbot application regarding the mentioned vulnerability, while mentioning the use of PyYAML version 5.1 for parsing YAML configuration files. The given global_knowledge identifies a vulnerability in PyYAML, but does not provide details about the chatbot application's security measures.\n",
      "\n",
      "Yes, there is a potential for arbitrary code execution if the application processes untrusted data like user-supplied input files in PyArrow versions 0.14.0 to 14.0.0, as mentioned in the given threat or vulnerability report. It is recommended to upgrade to PyArrow 14.0.1 or use the provided `pyarrow-hotfix` package for older versions to mitigate this issue. The chatbot application, however, operates with PyArrow 13.0.0, which is not directly affected by this vulnerability.\n",
      "\n",
      "Upgrade PyArrow to version 14.0.1 or use the provided `pyarrow-hotfix` package for older versions (before 14.0.1) to disable the vulnerability. Ensure downstream libraries have upgraded their dependency requirements to PyArrow 14.0.1 or later, if applicable. If upgrading is not possible, follow the instructions from the pyarrow-hotfix package at https://pypi.org/project/pyarrow-hotfix/.\n",
      "\n",
      "The potential impact of the c-ares denial-of-service vulnerability on your application could stem from its use of outdated version 1.19.0, which is vulnerable to the described attack. As the vulnerability allows an attacker to cause denial of service through malformed UDP packets, it might disrupt the normal functioning of your application's asynchronous DNS requests depending on the attack's effectiveness and targeting. Updating to version 1.19.1, which includes the patch, would mitigate this risk. More information required about the specific attack scenario and extent of exposure faced by your application to determine precise consequences.\n",
      "\n",
      "Considering your local_knowledge has Libuv at version 1.47.0 which is affected by the reported vulnerability, and the global_knowledge explains that the vulnerability can allow an attacker to craft payloads and bypass developer checks, potentially allowing access to internal APIs or resources, there is a possibility of such an incident occurring in your application setup. It is recommended to upgrade to the fixed version (1.48.0) as stated in the global_knowledge to mitigate this issue. More specific mitigation details dependent on your application structure might not be provided by the given context.\n",
      "\n",
      "With the given llhttp parser vulnerability in Node.js, our application facing potential HTTP Request Smuggling (HRS) attack scenarios includes situations where an adversary manipulates or injects incomplete or incorrect HTTP header boundaries, exploiting the fact that the CR character (without LF) is sufficient to delimit HTTP header fields in the llhttp parser contrary to RFC7230. Attackers may aim to mislead the server into processing requests incorrectly, combining parts of different requests, or causing unintended responses. More details about specific attack scenarios may require further analysis depending on the application design and usage context.\n",
      "\n",
      "Considering your application uses nghttp2 version 1.40.0, which is vulnerable to the described denial of service attack, an attacker could potentially exploit this vulnerability by sending repeated large SETTINGS frames. However, you have a workaround implemented in your system by dropping the connection when the received SETTINGS frame has a large number of settings entries (e.g., > 32), which mitigates this threat. To fully address the vulnerability, upgrade to nghttp2 version 1.41.0 or later, as it includes the fix for this issue. More information required about the specific attacker scenario and connection context to determine the exact feasibility.\n",
      "\n",
      "Based on the given information, our Faceapp application uses OpenSSL version 1.9.0 from the quictls/openssl fork, which focuses on QUIC support and closely tracks main OpenSSL releases. The vulnerability mentioned in the global_knowledge relates to an older version (up to 2.0.0) of the node-openssl package, which is not the same one used in our application. Therefore, it is less likely that our application is affected by the specific vulnerability of the node-openssl package stated in the global_knowledge. However, more comprehensive assessment of our application's security posture may require a thorough analysis beyond this context.\n",
      "\n",
      "More information is required about the vulnerability in question and its impact on OpenSSL versions beyond 2.0.0 to provide a definitive answer.\n",
      "\n",
      "The potential impact of the SpEL Injection vulnerability in the Spring Data MongoDB dependency, when using @Query or @Aggregation-annotated query methods with unsanitized input, includes allowing unauthorized query manipulation, data exposure, or even enabling remote code execution within the application, leading to significant security risks.\n",
      "\n",
      "Yes, since your application uses Spring Framework version 6.0.15 with Spring MVC and likely has Spring Security 6.1.6+ or 6.2.1+ on the classpath due to the dependency on spring-boot-starter-security, it falls under the conditions where it may experience a denial-of-service vulnerability due to specially crafted HTTP requests.\n",
      "\n",
      "The potential impact of the vulnerability in the JSON Web Tokens implementation using version 8.5.0 of jsonwebtoken library in your Spring Boot application is that it could result in incorrect verification of tokens, allowing forged tokens to be successfully validated. This is due to a misconfiguration issue that may permit a different algorithm and key combination during verification other than the one used to sign the tokens, particularly when using asymmetric and symmetric key combinations with the same key retrieval function. It is important to note that this issue has been patched in version 9.0.0, so an upgrade to this version should mitigate the risk. In the meantime, proper configurations and implementations should be ensured to avoid such vulnerabilities.\n",
      "\n",
      "Considering the given global_knowledge focuses on H2 Console vulnerability and not specifically on ModelMapper or its dependencies, it is crucial to note that more information is required to establish a connection between the vulnerability mentioned and the com.h2database dependency within the context of the resumelink application. Particularly, we need details on whether the vulnerable version of H2 Console (before 2.1.210) is indeed employed in the application, and if its interaction with the com.h2database dependency could lead to exploitation of the mentioned vulnerability. Without this information, it is not possible to accurately assess the potential impact of the com.h2database vulnerability on the application.\n",
      "\n",
      "The given vulnerability in the org.h2.util.JdbcUtils.getConnection method could pose a risk to the Spring Boot application if it utilizes H2 database and ModelMapper version lower than the fixed version, and if the attack vector specifically targets through the H2 Console or similar means. In such a case, unauthenticated remote code execution may occur due to improper validation of JNDI driver names and URLs, potentially leading to severe security implications. However, if the Spring Boot application employs the specified ModelMapper version 2.4.2 and addresses necessary security measures, the likelihood of exploitation through this vulnerability might be reduced. It's important to verify the actual version and configuration of H2 and ModelMapper dependencies in the application to fully assess the potential impact.\n",
      "\n",
      "More information is required to determine if the specific version of SQLite used in the toy event simulator application is affected by the identified heap use-after-free issue (CVE-2024-0232). The provided global_knowledge mentions SQLite, but the exact version and whether it aligns with the local_knowledge's version needs clarification.\n",
      "\n",
      "In the given local_knowledge, SQLite version 3 from the toy event simulator application is being used. According to the global_knowledge, the vulnerability to remote code execution exists in versions 3.6.14.1 through 3.41.2.1, which have already been fixed in version 3.41.2.2. As the specific version used in the toy event simulator application is not within the vulnerable range mentioned in the global_knowledge, it is not exploitable by the remote-code-execution vulnerability. However, it's important to keep software up-to-date and ensure the latest security patches are applied to maintain a secure environment. More information regarding the exact version of sqlite-jdbc used in the application might be necessary to confirm this answer with certainty.\n",
      "\n",
      "The given information focuses on an integer overflow vulnerability with gcc and g++ 3.3.3 and earlier versions. It does not specify if the mentioned vulnerability is related to buffer overflows. More information is required to determine if the version of g++ mentioned in the local_knowledge is vulnerable to buffer-overflows.\n",
      "\n",
      "Parsing a file may lead to disclosure of user information with an older version of Xcode before 14.0, after it is fixed in Xcode 14.0.\n",
      "\n",
      "The given global_knowledge discusses an injection issue being addressed in Xcode 14.1, but it does not specifically mention Xcode's vulnerability to code injection in other versions. More information is required to determine the vulnerability status of older Xcode versions.\n",
      "\n",
      "The given global_knowledge discusses Jenkins Xcode integration Plugin having vulnerability to XXE attacks in its earlier version 2.0.14 and below. However, it does not directly relate to the standalone Xcode application mentioned in the local_knowledge. Thus, more information is required to determine if Xcode itself can be exploited to perform XML-external-entity (XXE) attacks.\n",
      "\n",
      "Yes, before libxml2 version 2.9.5, there existed a use after free vulnerability that allowed a remote attacker to potentially exploit heap corruption via a crafted HTML page. However, the given local_knowledge mentions using a version greater than 2.7 in their toy event simulator application, which is after the affected version range. So, it depends on the specific version of libxml2 being used in the context. More information about the actual version used is required to determine the exact risk.\n",
      "\n",
      "Yes, Libxml2, when used in contexts like Red Hat JBoss Core Services or in recovery mode, has been reported to allow context-dependent attackers to cause a denial of service via a crafted XML document due to excessive stack consumption. This vulnerability is linked to an incorrect fix for CVE-2016-3627. However, without specific information regarding the exact use case and version of Libxml2 in the mentioned application, a direct correlation to the mentioned scenario cannot be established. More information is required to determine the applicability of this vulnerability to the given use case and version.\n",
      "\n",
      "There is a vulnerability in cmake where it leaves itself open to MITM attacks, which theoretically could enable remote code execution (RCE) if an attacker can position themselves in between the user and the remote server, and successfully replace the requested binary with an attacker-controlled binary during the download of binary resources over HTTP. This risk might be applicable to the overall infrastructure involving the use of cmake, but specific to the given application details, more information is required to determine if this vulnerability can be exploited.\n",
      "\n",
      "In the given global_knowledge, there is mentioned an untrusted search path vulnerability in CMake before 2.2.0-r1 on Gentoo Linux, which allows local users in the portage group to gain privileges. This vulnerability is related to RUNPATH in the context provided. However, the local_knowledge does not explicitly discuss the compatibility or specific version details of CMake used in the TES-3 application. More information is required to determine if the mentioned vulnerability is relevant to the current infrastructure of the TES-3 application.\n",
      "\n",
      "Yes, there is a threat of directory traversal vulnerability in JSZip before version 3.8.0, and your application uses jszip version 3.6.1. This version might be susceptible to such attacks via a crafted ZIP archive.\n",
      "\n",
      "An adversary could potentially exploit the jszip package in the given Toy Browser context by creating a manipulated ZIP file with filenames set to Object prototype values (e.g., __proto__, toString, etc). When the application processes this malicious ZIP file using the vulnerable version of jszip (before 3.7.0), it may result in a returned object with a modified prototype instance, allowing arbitrary changes to an object's prototype instance. Updating to a fixed version (3.7.0 or higher) would mitigate this issue.\n",
      "\n",
      "Considering the threat reported in global_knowledge where WebTorrent before 0.107.6 allows XSS in the HTTP server, and the toy browser application uses webtorrent version 0.105.6, there might be a risk of XSS on the HTTP server in this particular scenario. However, it's important to note that an upgrade to version 0.107.6 or above could potentially address this vulnerability. More information is required to confirm the exact version and context to provide a definitive answer.\n",
      "\n",
      "An attacker could exploit the vulnerability in JSON5 versions before 1.0.2 and 2.2.2 by providing a JSON string containing a key named '__proto__'. Upon parsing this input with `JSON5.parse()`, the prototype of the resulting object may be polluted with arbitrary and unexpected keys, potentially leading to severe security implications depending on how the returned object is used in the application. Updating to a patched version (1.0.2, 2.2.2, or later) of JSON5 mitigates this issue. In the provided local_knowledge, the utilized version is 1.0.1, which is vulnerable to this threat.\n",
      "\n",
      "Considering the given global_knowledge, prototype pollution vulnerability exists in function parseQuery in parseQuery.js in webpack loader-utils. Toy Browser using webpack does not explicitly specify the versions of loader-utils or if they are affected by this vulnerability. More information is required about the exact versions of loader-utils and their relation to the prototype pollution vulnerability to determine if the attacker can exploit this issue in Toy Browser.\n",
      "\n",
      "An adversary taking advantage of the vulnerability in terser package before specific versions (terser before 4.8.1, from 5.0.0 and before 5.14.2) could potentially exploit your application through Regular Expression Denial of Service (ReDoS) attacks. They may craft malicious input with a complex regular expression that causes terser to spend excessive time or resources during the minification process, thereby causing a denial of service. However, as your application uses terser version 5.14.0, which is not affected by this vulnerability, the risk is mitigated in your case.\n",
      "\n",
      "More information is required to describe the specific exploitation scenario in detail or to understand the exact impact it could have on your application's functionality.\n",
      "\n",
      "More information is required to determine if an XSS vulnerability exists in the context of the Toy Browser application specifically, considering only the given redux framework usage. The provided threat or vulnerability report mentions XSS in a different plugin unrelated to the mentioned Redux dependencies and versions in the Toy Browser's local_knowledge.\n",
      "\n",
      "More information is required to answer the question as it mixes concepts from global_knowledge about a specific vulnerability with local_knowledge that describes the use of various Redux libraries in a browser application, without mentioning any relation to MD5 hash or authentication.\n"
     ]
    }
   ],
   "source": [
    "prometheus_completions = get_aimaven_prometheus_7b_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "size = len(prometheus_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_7b_completion_file_path = \"./data/evaluation/LocalIntel_prometheus_7b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['prometheus_7b_completions'] = prometheus_completions\n",
    "    eval_df.to_excel(prometheus_7b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe given threat or vulnerability report highlights an issue in Harrison Chase langchain version v.0.0.194 and older, where a remote attacker can execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions. While the described vulnerability is related to LangChain, it is essential to note that the specific context of the chatbot application provided does not directly indicate its version or whether it falls within the vulnerable range. To determine the susceptibility of the chatbot application to this exploitation, more information about the actual version used in the application is required.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prometheus_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1 8B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.36it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "llama_31_completions = get_llama_31_8b_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "size = len(llama_31_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_8b_completion_file_path = \"./data/evaluation/LocalIntel_llama_31_8b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['llama_31_8b_completions'] = llama_31_completions\n",
    "    eval_df.to_excel(llama_8b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An issue in LangChain v.0.0.194 and before allows a remote attacker to execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_31_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral NeMo 12B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 53.69 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 2.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mistral_nemo_12b_completions \u001b[38;5;241m=\u001b[39m \u001b[43mget_mistral_nemo_12b_instruct_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_prompt_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/shaswata/4TB_2/Desktop_Backup/Context_CTI/utils/model_utils.py:180\u001b[0m, in \u001b[0;36mget_mistral_nemo_12b_instruct_completion\u001b[0;34m(prompts_list, max_tokens)\u001b[0m\n\u001b[1;32m    178\u001b[0m completion_request \u001b[38;5;241m=\u001b[39m ChatCompletionRequest(messages\u001b[38;5;241m=\u001b[39m[UserMessage(content\u001b[38;5;241m=\u001b[39mprompt)])\n\u001b[1;32m    179\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_chat_completion(completion_request)\u001b[38;5;241m.\u001b[39mtokens\n\u001b[0;32m--> 180\u001b[0m out_tokens, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m                         \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstruct_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m result \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(out_tokens[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    183\u001b[0m completions\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/localintel_2/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/localintel_2/lib/python3.11/site-packages/mistral_inference/generate.py:76\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(encoded_prompts, model, images, max_tokens, temperature, chunk_size, eos_id)\u001b[0m\n\u001b[1;32m     68\u001b[0m cache_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(seqlens) \u001b[38;5;241m+\u001b[39m max_tokens\n\u001b[1;32m     69\u001b[0m cache \u001b[38;5;241m=\u001b[39m BufferCache(\n\u001b[1;32m     70\u001b[0m     model\u001b[38;5;241m.\u001b[39mn_local_layers,\n\u001b[1;32m     71\u001b[0m     model\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     model\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m     75\u001b[0m )\n\u001b[0;32m---> 76\u001b[0m \u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m cache\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Bookkeeping\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/localintel_2/lib/python3.11/site-packages/mistral_inference/cache.py:141\u001b[0m, in \u001b[0;36mBufferCache.to\u001b[0;34m(self, device, dtype)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice, dtype: torch\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBufferCache\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 53.69 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 2.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "mistral_nemo_12b_completions = get_mistral_nemo_12b_instruct_completion(input_prompt_list[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral_nemo_minitron_8B_Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.31it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "mistral_nemo_minitron_8b_completions = get_mistral_nemo_minitron_8b_base_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "size = len(mistral_nemo_minitron_8b_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_nemo_minitron_8b_completion_file_path = \"./data/evaluation/LocalIntel_mistral_nemo_minitron_8b_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['mistral_nemo_minitron_8b_completions'] = mistral_nemo_minitron_8b_completions\n",
    "    eval_df.to_excel(mistral_nemo_minitron_8b_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The LangChain's chains used for implementing Program-Aided Language Models (PAL) within the chatbot application are susceptible to exploitation due to the issue in Harrison Chase langchain v.0.0.194 and before, which allows a remote attacker to execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions. This vulnerability can be exploited to inject malicious code into the chatbot's responses, potentially leading to unauthorized access or data theft. Therefore, it is recommended to upgrade to a more recent version of LangChain to mitigate this vulnerability.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mistral_nemo_minitron_8b_completions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, it is not possible for an adversary to predict the MD5 hash value in order to wrongly authenticate an unauthenticated user. The MD5 hash function is a cryptographic hash function that is designed to be one-way, meaning that it is computationally infeasible to reverse the hash function to obtain the original input. Therefore, an adversary cannot predict the MD5 hash value of a given input in order to wrongly authenticate an unauthenticated user.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mistral_nemo_minitron_8b_completions[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_completions = get_gpt_4o_completion(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "size = len(gpt_4o_completions)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_completion_file_path = \"./data/evaluation/LocalIntel_gpt_4o_Dataset.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if size == len(input_prompt_list):\n",
    "    eval_df['gpt_4o_completions'] = gpt_4o_completions\n",
    "    eval_df.to_excel(gpt_4o_completion_file_path)\n",
    "else:\n",
    "    print(\"Completion count mismatch! Error occured in generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The application's use of LangChain for implementing Program-Aided Language Models (PAL) is susceptible to exploitation due to the issue in LangChain version 0.0.194 that allows a remote attacker to execute arbitrary code via the from_math_prompt and from_colored_object_prompt functions.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4o_completions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset Desciption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.886879</td>\n",
       "      <td>18.178453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.250000</td>\n",
       "      <td>14.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>30.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.750000</td>\n",
       "      <td>45.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0\n",
       "count     58.000000   58.000000\n",
       "mean      28.500000   30.000000\n",
       "std       16.886879   18.178453\n",
       "min        0.000000    0.000000\n",
       "25%       14.250000   14.250000\n",
       "50%       28.500000   30.500000\n",
       "75%       42.750000   45.750000\n",
       "max       57.000000   60.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_prompt_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localintel_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
